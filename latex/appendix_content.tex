\appendix

\section{Appendix}

\subsection{Algorithm}
We formally present the \textsc{vertex addition} compression algorithm, using notation defined in the paper. $\ell$ linearizes a vertex set, based on left-to-right position in $S$. $|P|$ indicates the number of tokens in the priority queue.

\begin{algorithm}[]
\SetKwInOut{Input}{input}
\SetAlgoLined
\Input{$s=(V,E)$, $Q \subseteq V$, $b \in \mathbb{R^+}$}
 $ C \gets Q;  P \gets V \setminus Q$; \\
 \While{ $\ell(C) < b $ and $ |P| > 0 $   }{
  $v \gets \text{pop}(P)$; \\
  \If{$p(y=1) > .5$ and $\ell(C \cup \{v\}) \leq b$}{$C \gets C \cup \{v\}$}
 }
\KwRet{$\ell(C)$}
 \caption{\textsc{vertex addition}}
\end{algorithm}\label{a:algo}

\subsection{Reimplementation of \citet{filippova2013overcoming}}

In this work, we reimplement the method of \citet{filippova2013overcoming}, who in turn implement a method partially described in \citet{filippova2008dependency}.  There are inevitable discrepancies between our implementation and the methods described in these two prior papers.

\begin{enumerate}
\item{Where the original authors train on only 100,000 sentences, we learn weights with the full training set to compare fairly with \textsc{vertex addition} (which trains on the full training set.)}
\item{We use \citet{gurobi} (v8) to solve the liner program. \citet{filippova2008dependency} report using LPsolve.\footnote{\url{http://
sourceforge.net/projects/lpsolve}}}
\item{We implement with the common Universal Dependencies (UD, v1) framework \cite{Nivre2016UniversalDV}. Prior work \cite{filippova2008dependency} implements with older dependency formalisms \cite{briscoe-etal-2006-second,Marneffe2006GeneratingTD}.} 
\item{In Table 1 of their original paper, \citet{filippova2013overcoming} provide an overview of the syntactic, structural, semantic and lexical features in their model. We implement every feature described in the table. We do not implement features which are not described in the paper.}
\item{\citet{filippova2013overcoming} augment edge labels in the dependency parse of $S$ as a preprocessing step. We reimplement this step using off-the-shelf augmented modifiers and augmented conjuncts available with the enhanced dependencies representation in CoreNLP \cite{Schuster2016EnhancedEU}.}
\item{\citet{filippova2013overcoming} preprocess dependency parses by adding an edge between the root node and all verbs in a sentence.\footnote{This step ensures that subclauses can be removed from parse trees, and then merged together to create a compression from different clauses of a sentence.} We found that replicating this transform literally (i.e. only adding edges from the original root to all tokens tagged as verbs) made it impossible for the ILP to recreate some gold compressions. (We suspect that this is due to differences in output from part-of-speech taggers.) We thus add an edge between the root node and \textit{all} tokens in a sentence during preprocessing, allowing the ILP to always return the gold compression.}
\end{enumerate}

We assess convergence of the ILP by examining validation F1 score on the traditional sentence compression task. We terminate training after six epochs, when F1 score stabilizes (i.e. changes by less than $10^{-3}$ points).

\subsection{Implementation of SLOR}

We use the SLOR function to measure the readability of the shortened sentences produced by each compression system. Following \cite{lau2015unsupervised}, we define the function as 

\begin{equation}
\text{SLOR}=\frac{\text{log}P_m(\xi) - \text{log}P_u(\xi)}{|\xi|}
\end{equation}

where $\xi$ is a sequence of words, $P_u(\xi)$ is the unigram probability of this sequence of words and $P_m(\xi)$ is the probability of the sequence, assigned by a language model.  $|\xi|$ is the length (in tokens) of the sentence.

We use a 3-gram language model trained on the \citet{filippova2013overcoming} corpus. We implement with KenLM \cite{Heafield-kenlm}.

\subsection{Latency evaluation}
To measure latency, for each technique, we sample 100,000 sentences with replacement from the test test. We observe the mean time per sentence using Python's built in \textit{timeit} module. Code for extracting edge features is shared across both methods, to ensure that each technique incurs very similar latency penalties for edge feature extraction. We use an Intel Xeon processor with a clock rate of 2.80GHz. 

\subsection{Query sampling}
We sample queries for our synthetic constrained compression experiment to mimic real-world searches: the distribution of query token lengths and the distribution of query part-of-speech tags employed in our experiment closely match empirical distributions observed in real search \cite{Jansen2000RealLR,Barr2008TheLS}. To create queries, for each sentence in our corpus we: (1) sample a query token length in proportion to the real-world distribution over query token lengths (2) sample a proper or common noun in proportion to the real-world distribution over proper and improper nouns in queries, (3) add a noun or proper noun from the gold compression $C_g$ (based on step 2) to the set $\{Q\}$, if such a token exists (4) repeat steps 2 and 3 until $\{Q\}$ is the length specified in step 1. We exclude sentences in cases where the gold compression $C_g$ does not contain enough nouns to fill $\{Q\}$ to the desired length.

\subsection{Neural network tuning and optimization}
We tune hyperparameters for \textsc{vertex addition}$_{NN}$ via random search \cite{Bergstra2012RandomSF}. We select parameters which optimize for vertex-level accuracy on oracle decisions for the validation set. The hyperparameters are: the hidden state size of the LSTM, the learning rate, the dimensionality of embeddings and the weight decay parameter. During training, we minimize Cross-Entropy loss with \textsc{AdaGrad} \cite{duchi2011adaptive} for 10 epochs.

\begin{table}[htb!]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Learning rate  & A \\ 
Embedding dim. &  A \\
Weight decay   &  A\\
Hidden dim.    &  A \\
\end{tabular}
\end{table}

\bibliography{abe}
\bibliographystyle{acl_natbib}