%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Extractive sentence compression under lexical and length constraints}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Outline}

\begin{enumerate}
\item{$(q,s,r)$ compression is useful.}
\item{How do you do it? ILPs, neural net taggers, iterative deletion supervised w/ people or w/ gold data? (\S\ref{s:method}).}
\item{Why iterative deletion is good. Description of current system for making deletion decisions w/ human supervision. Possibly make this system better too}
\item{Computational experiments (1): Properties of q,s,r compression in English. }
\item{Computational experiments (2): F and A data, interpreted as q,s,r supervision/data. Enumerate methods and bold the one with the best token-level F1 score.}
\item{Possible post hoc human experiment? Is our system actually good?}
\end{enumerate}




\section{Why iterative deletion is good}\label{s:method}
\begin{enumerate}
\item{Historical reasons}
\item{grammatical bias + syntax bias + linguistically-motivated? Leverages a lot that is know from linguistics: e.g. adjuncts, transitive bias, function words.}
\item{budget and query are trivial}
\item{linear (taggers) vs. quadratic (this work) vs. ILPs (exponential). }
\item{single op supervision can be used}
\item{avoid computational waste w/ grammar! HUGE divergence between mathematical properties of compression problem (exponential) and ways you can actually compress a sentence.}
\item{One thought: you could do a neural system that decides when/what to delete, using F and A as supervision? Each tree gets an encoding and you do a pointer network style softmax over trees? This sounds hard to implement.}
\end{enumerate}

\subsection{Preprocessing with extract}
Extract preprocessing. Make up a few simple rules. They will help a bunch. Only like 6 dependency types work at all for extract. This is in realfake/createdata/extract at the moment.

\section{Computational experiments part 1: investigating properties of q,s,r compression}
\begin{enumerate}
\item{q = a list of 1 to 3 NER}
\item{r = random}
\item{What is the size of the minimum compression?}
\item{Reachability by budget by position of q in syntax tree. (If q is more than one entity then how the entities are dispersed across the tree probably matters a bunch too).}
\item{Hang on. reachability == min compression, eh? if min compression $>$ b, it is clearly bad.}
\item{Avoid computational waste w/ grammar.  Examine: ops you never have to worry about if you prune a branch v. dependency type deletion endorsement rate. Some ops get rid of lots of tokens w/ very high probability of deletion endorsements: e.g. parataxis (a great op!). By contrast: pruning a noun subj destroys acceptability and usually does not delete many tokens. Not worth the risk!}
\item{What is the empirical number of ops (i.e. decisions you have to make about pruning) if you greedily drop branches but never drop if the single op probability is less than $p$? My guess is you can make this problem way, way, simpler than implied by exponential formulation. Is it really quadratic?}
\item{Distribution of number of ops used for different q and r: when choosing ops at random? when choosing greedily? When pruning $\propto$ p(endorsement)?}
\item{Other stuff: min compression, reachability, operations saved w/ big prunes? position of query in the tree?}
\end{enumerate}

\section{Computational experiments part 2: sentence compression}
Interpret F and A as Q,S,R supervision/data. Then test methods which do $(q,s,r)$ compression. Measure w/ token level F1.


\begin{table}[htb!]
\begin{tabular}{ll}
\centering
Approach & F1 \\ \hline
Human acceptability supervision         &  A          \\
F and A (2013)    & B           \\
C and L (2008)    & C        \\
Neural subtree deletion methods? &  D    \\   
\end{tabular}
\end{table}

\begin{enumerate}
\item{q = a list of 1 to 3 NER which are in the compression. All the NER in the compression? If the compression has no NER, use noun unigrams I guess? or probably just skip it}
\item{r = length of gold}
\end{enumerate}


\section{TODO}
\begin{enumerate}
\item{Look at query-biased snippets in IR. There is an old email about this. Look a. Ask Hamed and John. JM chap 23 2ed does not really have cites for this.}
\item{Find old ILP code and get stuff working}
\item{How will we deal with semantics? Brendan email about subsective adjectives ellie pavlik work. }
\end{enumerate}

\section{Related work}

Traditional study of sentence compression is closely aligned with research on text summarization  \cite{Knight2000StatisticsBasedS}: classical approaches to summarization focus on selecting and (sometimes) shortening sentences \cite{vanderwende2007beyond,Nenkova2012ASO}. The traditional goal of retaining the most ``important'' information from a source sentence during compression \cite{filippova2008dependency,clarke2008global,filippova2013overcoming} reflects this intended use case; shorter sentences should stand in for longer sentences in summaries.


(Our interest in this problem arose while developing a practical, custom search system.)

Length and lexically constrained compressions could also be used as part of new search user interfaces \cite{hearst2009search}, such as concept map browsers \cite{falke2017graphdocexplore}.

\bibliography{abe}
\bibliographystyle{acl_natbib}

\end{document}
