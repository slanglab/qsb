%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{graphicx}

\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\newcommand{\rdep}[1]{\ $\xrightarrow{\text{\tiny #1}}$\ }


\newcommand{\ahcomment}[1]{\textcolor{blue}{[#1 -AH]}}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\hypersetup{draft} % THIS IS NEEDED TO GET IT TO COMPILE. Does not like the tables. AH 11/27

\newcommand\BibTeX{B{\sc ib}\TeX}

\DeclareMathOperator*{\argmaxA}{arg\,max} % Jan Hlavacek

% Extractive sentence compression under lexical and length constraints
\title{Transition-based sentence compression with lexical and length constraints}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

% \ahcomment{Proposed revision: In this work we present a new, transition-based framework for extractive sentence compression. We use this framework to construct a sentence compression system based on a very simple neural network, which achieves large gains in automatic evaluation measures over previous state-of-the-art approaches. Unlike prior techniques, our method also allows accommodates lexical and length constraints without incurring exponential computational costs. This makes our method better suited to user-facing search applications, where length and lexical constraints are paramount.}

% \ahcomment{note: gurobi pool search mode is turned ON which makes everythign go slower. you should turn this off if you start making arguments about wall clock time}


\begin{abstract}
Traditional approaches to extractive sentence compression seek to reduce the length of a sentence, while retaining the most ``important'' information from the source. But query-focused applications such as document search engines or exploratory search interfaces place additional lexical and length requirements on compression systems. This study introduces a new transition-based compression method which can accommodate such requirements.  Our technique is more computationally efficient than classical ILP-based approaches, and more accurately reconstructs known-good shortenings under constraints. 
\end{abstract}

\section{Introduction}

Traditional study of extractive sentence compression seeks to create short, readable compressions which retain the most ``important'' information from source sentences. But query-focused and user-facing applications impose additional requirements on the output of a compression system. Compressions must be short enough to be shown in a user interface and often must contain a user's query term, which defines the ``important'' information in the sentence. An example of such a compression is shown in Figure \ref{f:qf}.


\begin{figure}[htb!]
\fbox{
\begin{minipage}{7.6 cm}
\includegraphics[width=7.5cm]{qf.pdf}
\caption{Search interfaces often require compressions with length and lexical constraints. In this case, a system returns a related link (top) above a constrained compression from a retrieved document (bottom). The constrained compression must contain the users' query terms (shown in bold), and must be shorter than the maximum length constraint of 60 characters.}
\label{f:qf}
\end{minipage}
}
\end{figure}

This study examines the English-language compression problem with such length and lexical requirements: compressions which must include a list of query words $q$, and must have a character length which is less than or equal to $b \in \mathbb{Z}^{+}$. While older compression methods based on integer linear programming could trivially accommodate such restrictions \cite{clarke2008global,filippova2013overcoming}, recent work has focused on neural network techniques \cite{filippova2015sentence} which achieve similar performance without handcrafted linguistic features, but which do not give practitioners such control. This makes existing neural methods unsuitable for search engines \cite{hearst2009search}, concept map browsers \cite{falke2017graphdocexplore} and new forms of exploratory textual interfaces \cite{marchionini2006exploratory}, where length and lexical constraints are paramount. 

\begin{table*}[htb!]
\begin{tabular}{lccc}
\textbf{Approach} & \textbf{Worst-case complexity} & \textbf{Constrained}  \\ \hline
LSTM taggers \cite{filippova2015sentence}   & linear              & no         \\   
\textbf{Iterative deletion (this work)}  & \textbf{quadratic}     &      \textbf{yes}   \\
ILP    \cite{filippova2013overcoming,Wang2017CanSH}       &   exponential    & yes      \\
\end{tabular}
\caption{Three algorithms for sentence compression. Linear programming methods \cite{clarke2008global,filippova2013overcoming,Wang2017CanSH} can easily accommodate length and lexical constraints, and can make use of large parallel corpora of sentence--compression pairs. But these methods formulate the compression task as an NP-complete optimization problem (\S\ref{s:ilps}), with worst-case exponential runtime. LSTM taggers \cite{filippova2015sentence} achieve comparable results with a linear runtime, but cannot accommodate length or lexical requirements. This work introduces a supervised, transition-based approach (\S\ref{s:system}) which can be used to compress sentences under lexical and length constraints in quadratic time.} 
\label{t:algos}
\end{table*}


Therefore, in this work, we present a new method for compressing sentences, which efficiently accommodates lexical and length constraints. Our transition-based, stateful compression method is inspired by similar approaches to transition-based dependency parsing \cite{nivre2003,D14-1082}. We compare this transition-based technique to traditional ILP-based methods, which also accommodate length and lexical restrictions. We show that our method has lower computational costs while better reconstructing known-good, constrained shortenings, as measured by token-level F1 score. 

\section{Compression and constraints}

This work contrasts traditional ILP-based compression with our novel transition-based framework. We also briefly discuss compression with LSTM taggers, which do not accommodate length or lexical requirements. Table \ref{t:algos} provides a summary of the strengths and weaknesses of each method.

\subsection{Constrained compression with ILPs}\label{s:ilps}

One common approach for shortening sentences formulates compression as an integer linear programming (ILP) task. ILP-based methods assign binary variables to each token in a sentence \cite{clarke2008global} or subtree in a dependency parse \cite{filippova2008dependency}. These variables indicate if the corresponding component included in a compression. Each such sentence component is also assigned a local weight, indicating its worthiness for inclusion in a shortened sentence. Local weights are either learned from direct supervision \cite{filippova2013overcoming,Wang2017CanSH}, or inferred from sources like corpus statistics, importance score metrics and n-gram language models \cite{clarke2008global,filippova2008dependency}.

Such ILP methods represent the overall quality of a compression by summing the local weights of sentence components to compute a global objective score.  The problem of identifying the best possible solution to an integer programming compression objective is exponential in the number of tokens or number of subtrees in the input, as each binary variable may be set to zero or one. Researchers use off-the-shelf ILP solvers to identify the highest-scoring compression, from among the exponential possible configurations of binary variables.

This integer linear programming approach also easily accommodates constrained compression. Researchers will customarily add constraint terms to the ILP objective to enforce hand-build semantic restrictions \cite{clarke2008global} or syntactic requirements \cite{filippova2008dependency}. With such methods, adding additional length or lexical requirements is trivial: practitioners must specify that optimal solutions must be shorter than some character budget, and must specify that binary variables marking inclusion of particular words must be set to 1. 

\subsection{Transition-based constrained compression}

In this work, we present an alternative, transition-based method for shortening sentences under lexical and length constraints, inspired by transition-based parsing techniques \cite{Earley1970AnEC,nivre2003}. Our method compresses a sentence over $N$ time steps, by adding and removing $N$ different subtrees from a dependency parse, one after another. This method recalls early solutions to the compression problem \cite{Jing2000SentenceRF,Knight2000StatisticsBasedS}, which also shortened sentences by executing grammatically-motivated operations on syntax trees. We present the formal details in Section \ref{s:system}. 

Like ILP-based methods, transition-based approaches can easily accommodate lexical and length restrictions. At a high-level, such methods need to add subtrees which contain query terms and remove subtrees which do not contain query terms, until identifying a compression which satisfies the length constraints. However, we show that transition-based methods have lower computational costs (\S\ref{s:costs}).

\begin{table*}[]
\centering
\begin{tabular}{llp{70mm}}
\textbf{Operation} &             \textbf{Definition}                                                    &      \textbf{Description}    \\ \hline
\textsc{Start}      & START $\Rightarrow ( V=\emptyset,  B=[v_1, v_2 ... v_n])$ & Initialize the buffer with the vertexes in the original sentence $s$, arranged breadth-first \\ \hline
\textsc{Prune}              & $(V, [v|B]), v \in V,  \Rightarrow (V \setminus  T(v), B)$ & Remove the subtree rooted at $v$ in $s$ from $V$ \\  
$\textsc{Insert}$             & $(V, [v|B]), v \notin V, \Rightarrow (V \cup T(v), B)$ & Insert the subtree rooted at $v$ in $s$ into $V$  \\ \hline
\textsc{NoPrune}           & $(V, [v|B]), v \in V, \Rightarrow (V, B)$ & Don't remove the subtree rooted at $v$ from $V$  \\ 
\textsc{NoInsert}          &       $(V, [v|B]), v \notin V, \Rightarrow (V, B)$ &   Don't insert the subtree rooted at $v$ into $V$    \\ \hline
\textsc{Stop}             & $ (V, B=[]) \Rightarrow$ STOP & Compression ends when the buffer is empty \\                                               
\end{tabular}
\caption{A transition-based sentence compression system with a \textsc{Prune} and \textsc{Insert} operation. The state of the compression system is a tuple $(V, B)$, where $B$ is an ordered buffer of tokens and $[v|B]$ indicates that $v$ is at the head of the buffer. $V$ is a subset of vertexes from the original sentence $s$. $T(v)$ denotes the subtree rooted at $v$ in the original sentence. The \textsc{Prune} operation removes all tokens in $T(v)$ from $V$. The \textsc{Insert} operation adds all vertexes from $T(v)$ into $V$. If the compression system does not execute a \textsc{Prune} or \textsc{Insert}, $v$ is removed from the head of the buffer and $V$ is not modified. $B$ is initialized with all tokens from $s$ (arranged in breadth-first order) at the \textsc{Start} of compression. Once the buffer is empty, compression will \textsc{Stop} and all vertexes in $V$ are linearized in their original order. These operations can fully reconstruct all shortenings in a standard compression corpus \cite{filippova2013overcoming}. The appendix presents a complete, worked example.}
\label{t:ops}
\end{table*}

\subsection{Unconstrained compression with LSTMs}

We contrast transition-based compression and integer programming approaches with LSTM taggers for the compression task \cite{filippova2015sentence}. Such taggers achieve state-of-the-art scores on extractive sentence compression, using sequence-to-sequence models that label tokens with a 1 or a 0 indicating if the token should be included in a summary. This approach is linear in the token length of a the input sequence. However, at this time, LSTM taggers are unsuitable for query-focused applications because such methods cannot enforce lexical or length requirements. This limitation might be examined in future research.

\section{Transition-based sentence compression}\label{s:system}

In this work, we present a new transition-based sentence compression system. We formally describe the system (\S\ref{s:formal}), present a method for generating oracle paths to gold standard shortenings (\S\ref{s:oracle}), and then detail a computational model of oracle compression (\S\ref{s:modeling}). We then demonstrate how to use this model for constrained compression (\S\ref{s:transition}), and analyze its performance.

\subsection{Formal description}\label{s:formal}

Our transition-based sentence compression system shortens a sentence $s$ by executing a sequence of operations. Each operation modifies the state, denoted $(V,v|B)$, where $V$ is a subset of tokens from $s$, and $B$ is an ordered buffer of tokens from $s$ headed by the token $v$. In defining each operation, we use the notation $T(v)$ to refer to the subtree headed by $v$ in the unchanging, precomputed dependency parse of $s$.

We define four principal operations, which reference the vertex $v$ at the head of the buffer, $v|B$. The operation \textbf{\textsc{Prune}} removes all vertexes in $T(v)$ from $V$. The operation \textbf{\textsc{Insert}} adds $T(v)$ into $V$. \textsc{Prune} and \textsc{Insert} each pop the token $v$ from the head of the buffer. The compression system can also execute the operations \textbf{\textsc{NoPrune}} and \textbf{\textsc{NoInsert}}, which pop $v$ from the buffer without inserting or pruning $T(v)$. 

To generate a compression, we first execute the $\textsc{Start}$ operation which sets $V=\emptyset$ and defines $B=[v_1, v_2 ... v_{|s|}]$, where $v_1, v_2 ... v_{|s|}$ are the tokens in $s$, arranged breadth-first, and $|s|$ is the token length of the original sentence. After the buffer is empty, we execute a $\textsc{Stop}$ operation and all tokens in $V$ are linearized in their original order to return a shortened sentence. 

Table \ref{t:ops} formally defines all operations. The appendix also includes a complete, worked example. 

In our transition-based compressor, only some operations are valid for some configurations of the state. If $v \notin V$, then $\textsc{Prune}$ is not valid as $T(v)$ cannot be removed from $V$. Similarly, if $v \in V$, then $\textsc{Insert}$ is not valid as $T(v)$ cannot be added to $V$. Table \ref{t:ops} includes these preconditions in the definition of each operation.

\subsection{Oracle paths}\label{s:oracle}

We identified the operations in our compression system empirically: we found that for all compressions in a large, standard corpus \cite{filippova2013overcoming} there exists an oracle path of at most $|s|$ operations which can fully reconstruct the shortened sentence. $|s|$ denotes the token length of the original sentence.

We identify the oracle path by executing  a sequence of operations. Let $(V_j, v_j | B)$ denote the state at timestep $j$. The oracle operation at step $j$ is unambiguously determined by $c_g$, the vertexes in the gold compression. For instance, if $v_j \in V_j$ but $v_j \notin c_g$, then the oracle must execute \textsc{Prune}: $B$ is arranged breadth-first, so our system can only remove $v_j$ at timesteps $j^{\prime} \leq j$. Similarly, if $v_j \in c_g, v_j \in V$ then the oracle operation at timestep $j$ must be $\textsc{NoPrune}$. If the system were to prune $v_j$ at timestep $j$, it will not have the opportunity to insert the vertex at a later timestep. We use analogous reasoning to identify oracle $\textsc{Insert}$ and $\textsc{NoInsert}$ operations. By executing each oracle operation in sequence, we compress $s$ to $c_g$ and identify the oracle path.

Because it is only possible to prune $v_j$ if $v_j \in V$, and only possible to insert $v_j$ if $v_j \notin V$ (\S\ref{s:formal}), we interpret the oracle path as a sequence of binary decisions. If $v \in V$, the compression system must decide to execute $\textsc{Prune}$ or $\textsc{NoPrune}$. If $v \notin V$, it must decide to execute $\textsc{Insert}$ or $\textsc{NoInsert}$. We use this interpretation of the oracle path during modeling (\S\ref{s:modeling}).


\subsection{Modeling}\label{s:modeling}

We use a large corpus of sentence--compression pairs \cite{filippova2013overcoming} to train an LSTM to predict $p(y=1 | \bm{x})$, the probability of a binary oracle decision (\S\ref{s:oracle}), given a collection of input symbols $\bm{x}$.

Loosely following \citet{D14-1082}, we define a set of tuples $\{(\bm{x}_j, y_j) \}_{j=1}^{K}$, where $\bm{x}$ is a sequence of symbols reflecting some configuration of the compression system at some step $j$ on some oracle path and $y_j \in \{0,1\}$ is a binary variable indicating the oracle choice at timestep $j$. 

We use a markup function, $m(V,v,o)=\bm{x}$ to generate the sequence of input symbols for the LSTM. This function ``shows'' the model what a compressed sentence would ``look'' like if the compression system were to execute the operation $o \in \{\textsc{Prune}, \textsc{Insert}\}$ given the state $(V, v|B)$. In the case of insert operations, the markup ``shows'' what the resulting compression would be, if the subtree $T(v)$ were to be copied  into $V$. In the case of prune operations, the function ``shows'' what the compression would be if $T(v)$ were to be removed from $V$.

More concretely, $\bm{x}$ is the symbol sequence [\texttt{SOS}, $V_L$, $\langle {o \cdot d} \rangle$, $T(v)$, $\langle{/o\cdot d}\rangle$, $V_R$, \texttt{EOS}], where:

\begin{itemize}
\item{$T(v)$ denotes all vertexes in the subtree rooted at $v$ in the original sentence $s$, linearized in their original order.}
\item{The symbols \texttt{SOS} and \texttt{EOS} are special markers denoting the start and end of the sequence.}
\item{$V_L$ is all vertexes from $V$ which occur to the left of $T(v)$ in $s$, linearized in their original order. $V_R$ is identical to $V_L$, except that vertexes in $V_R$ must occur to the right of $T(v)$.}
\item{The symbol $\langle o \cdot d \rangle$ is formed by concatenating two different markers: (1) a start symbol of type $o$, indicating the beginning of the sequence of tokens affected by the operation $o$, and (2) a marker indicating $d$, the dependency type governing $T(v)$. For instance in the sequence shown in Figure \ref{f:example}, $\langle o, d \rangle$ is $\langle \textsc{Prune} \texttt{dobj} \rangle$. The symbol $\langle / o \cdot d \rangle$ is formed identically, only the marker indicates the end of the sequence.\footnote{The symbols $\langle {o \cdot d} \rangle$ allow us to encode some aspects of syntax trees using a vanilla LSTM \cite{Vinyals2015GrammarAA,Aharoni2017TowardsSN}, without the added complexities of explicitly encoding nested structures into the architecture of the network \cite{Tai2015ImprovedSR,Dyer2016RecurrentNN}.}}
\end{itemize}


\begin{figure*}[htb!]
\centering
\includegraphics[width=.75\textwidth]{example.pdf}
\caption{Our Bi-LSTM classification model, which learns to make a binary decision based on the markup, $\bm{x}$ (\S\ref{s:modeling}). The model uses a shared Bi-LSTM and max pooling layer, along with two separate fully-connected layers. One layer is responsible for binary \textsc{Prune} decisions, the other for binary \textsc{Insert} decisions. The appropriate binary choice (prune decision vs. insert decision) is always clear from context (\S\ref{s:oracle}). The markup $m(V,v,o)=\bm{x}$ ``shows'' the LSTM a proposed change to the state of the compression system.  In this case, the model must decide whether or not to \textsc{Prune} the subtree headed at $v$ = \textit{limiting}, governed by a $\texttt{dobj}$ relation. The tokens in $V$ (\S\ref{s:formal}) are displayed in bold in the original sentence (top).}
\label{f:example}
\end{figure*}

For instance, in Figure \ref{f:example}, $\bm{x}$ ``shows'' the  effect of a proposed \textsc{Prune} of the subtree governed by $v=$\textit{risk}. Tokens in $V$ are shown in bold. The symbols, $\langle \textsc{Prune} \texttt{dobj} \rangle$  and $\langle / \textsc{Prune} \texttt{dobj} \rangle$ mark the start and end of the span of tokens in $T(v)$.

Our architecture follows recent work on LSTM classification for sentence-level tasks \cite{D17-1070}. Specifically, we predict the binary, oracle operation using a Bi-LSTM layer, a max pooling layer, and a fully-connected classification layer. We maintain two separate fully-connected layers: one for the \textsc{Prune} operation and one for the \textsc{Insert} operation. We interpret the shared Bi-LSTM layer as learning deep features from the markup \textbf{x}, and each fully-connected layer as using those features to perform a different kind of binary classification. Figure \ref{f:example} shows the architecture of the network. Word vectors are updated during training.

We initialize with GloVe embeddings \cite{pennington2014glove} and train using weighted cross entropy loss with Adam \cite{Kingma2014AdamAM}. We weight the cost function in proportion to the prevalence of each class in our training set.\footnote{Specifically, we weight each training instance $(\bm{x}, y_i)$ using $\frac{T_k}{2 * T_{k,i}}$, the default class weighting scheme in Scikit-learn \cite{Pedregosa:2011:SML:1953048.2078195}. $T_k$ is the total number of training examples of operation type $k$ (e.g. $T_k$ = total number of \textsc{Prune} examples + total number of \textsc{NoPrune} examples). $T_{k,i}$ is the total number of training operations labeled $y_i$ for operations of type $k$ (e.g. $T_{i,k}$ = the total \textsc{NoPrune} operations, if $y_i=0$ and instance is of type \textsc{Prune}). 2 is the total number of classes. Alternative weighting methods are left for future work.} We train for 20 epochs, with early stopping if validation accuracy does not improve within 2 epochs. We use the network parameters from the highest-performing epoch for all evaluations.

Our model includes several hyperparameters including: the width of the max pooling layer, the learning rate, the Adam weight decay setting, the activation function, the dropout rate in the fully-connected layers and the dimensionality of input embeddings.\footnote{We also experimented with ELMo vectors \cite{Peters:2018}, but found that they slowed training dramatically. We were able to achieve similar validation accuracies with much smaller embeddings, and so did not pursue further work with ELMo. It is possible that ELMo-like vectors could be used to increase performance in the future.} 

We tuned all of these parameters on varying sized subsets of the training data by first searching coarsely and at random \cite{Bergstra2012RandomSF} over the parameter space; and then searching finely and at random over smaller regions of the parameter space which achieve high decision-level accuracy. The learning rate and weight decay parameter each have clear, observable effects on validation accuracy. 
The importance of other parameters is less clear. The final parameters are included in a table in the appendix. We implement with AllenNLP \cite{Gardner2017AllenNLP}.

\section{Experiment: Automatic Evaluation of Constrained Compression}\label{s:autoeval}

We use a large, standard dataset of sentence--compression pairs from \citet{filippova2013overcoming} to evaluate different approaches to length and lexically constrained compression. Using the dataset in this manner requires reinterpreting gold standard data, which does not specify either a query or length constraint. After re-tokenizing, parsing and tagging NER spans from the original dataset with Stanford CoreNLP 3.8.0 \cite{corenlp}, we define all NER tokens\footnote{We use a standard 3-class definition of NER; for our purposes, entities are people, locations or organizations.} which lie within the gold compression as the list of query tokens, $q$. We also define the character length of the gold-standard compression as the character budget, $b$. This interpretation allows us to redefine the corpus of $(s,c)$ pairs as a corpus of 4-tuples $(q,s,b,c)$, where $q$ is the query, $s$ is the original sentence, $b$ is the character budget and $c$ is some known-good compression which satisfies the length and lexical constraints. 

We then use token-level F1, to measure how well an ILP-based method (\S\ref{s:ilp}) and a transition-based method (\S\ref{s:transition}) method can reconstruct each known-good compression $c$, given $q,s$ and $b$. Token-level F1 is the standard automatic evaluation metric for the sentence compression task. \ahcomment{TODO: human eval}. We describe each method in the following sections.
    
\begin{table}[htb!]
\begin{tabular}{ll}
\centering
Approach & Constrained F1  \\ \hline
Query terms only (lower bound) & 0.384    \\
Supervised ILP  & 0.810           \\
 \textbf{Iterative deletion} &  \textbf{0.897}    \\
\end{tabular}
\caption{The F1 score for our iterative deletion approach to length and lexically constrained sentence compression, along with the F1 score for our reimplementation of a traditional ILP-based approach \cite{filippova2013overcoming}. Our method achieves an F1 score which is nine points higher. Selecting only query terms for a compression achieves an F1 = 0.384, which is the lower bound for this task.}
\end{table}

\subsection{Implementation: transition-based compression}\label{s:transition}

Our model of transition-based sentence compression (\S\ref{s:modeling}) predicts the oracle operation $y$, given the state $(V,v|B)$. There are many possible ways to use $p(y=1| \bm{x})$ in a query-focused and length-constrained compression system. In this work, we use a simple \textbf{iterative deletion} technique: at each step $j$ we \textsc{Prune} the subtree rooted at 

$$\argmaxA_{v \in V,q\not\in T(v)}   p(y = 1 | \bm{x})$$

\noindent where $\bm{x}$ is the markup and $y$ is the binary decision to perform a \textsc{Prune} or \textsc{NoPrune} operation (\S\ref{s:modeling}). We continue pruning subtrees until the length of the linearized tokens in $V$ is less than $b$, in which case we stop compression. 

We initialize $(V, v|B)$ with the smallest subtree from the dependency parse of $s$ which is (1) rooted at a verb and (2) contains all of the tokens in $q$. For roughly two-thirds of sentences $V$ is simply equal to all tokens in the original sentence. In the remaining cases, all tokens in $q$ are contained in some sub-clause of the sentence; the compression is formed by shortening this subclause, instead of shortening the whole sentence. In more than 95\% of sentences, the compression system must make additional \textsc{Prune} decisions after removing the smallest subtree.

This \textsc{Prune}-only iterative deletion is computationally simpler than ILP-based techniques. In the worst case, an iterative deletion method incurrs quadratic cost, but in practice costs are linear (\S\ref{s:empiricalcost}).

\subsection{Implementation: ILP-based compression}\label{s:ilp}

We compare our transition-based compression system to the state-of-the-art ILP-based method, presented in \citet{filippova2013overcoming}. To our knowledge, a public implementation of this method does not exist. We reimplement from scratch, achieving a similar a token-level F1 score as the original authors \ahcomment{.75, roughly. Add final number}. We use the leading \citet{gurobi} system (v7) to solve the ILP.

There are some important differences between our implementation and existing work, which arise in part because we use Universal Dependencies (v1) instead of the older dependency system employed in prior work. We detail these differences in the appendix. Following \citet{filippova2013overcoming}, we set the weights of the ILP with a structured perception trained on the 100,000 sentence--compression pairs from the corpus.

\section{Experiment: Human Evaluation of Constrained Compression}\label{s:humaneval}

\ahcomment{Standard likert scale readability / informativeness w. the 100 sentences. Complain about power analysis and stat sig in compression papers, including this one.}

\section{Computational costs of transition-based compression}\label{s:costs}

ILP-based approaches to sentence compression formalize the task as a linear programming optimization task, a well-known NP-complete problem with exponential worst-case complexity (\S\ref{s:ilps}). One advantage of our transition-based framework is that it can perform query-focused, budget-constrained compression in worst-case quadratic time. 

We define one unit of computational cost to be the computation required to evaluate one subtree for possible pruning. This cost scales with the token length of the sentence: the more tokens in a sentence, the more subtrees must be considered for deletion.  %One unit of computational cost i i.e. the cost of computing $p(y=1 | \bm{x})$. 

In the worst-case, our method will prune one singleton subtree (i.e. a subtree with one vertex) at each of the $N$ timesteps during compression.\footnote{Intuitively, pruning in this manner would remove a leaf from the (modified) parse tree at each timestep.} If a dependency parse of $s$ contains $|s|$ original vertexes, and $|s_i|$ refers to the token length of the remaining sentence at timestep $i$, then the iterative deletion method requires at most ${\sum_{i = 0}^{|s|} |s_i | = O(|s|^2)}$ possible operations, where it is always the case that $|s_{i + 1}| = |s_{i}|  - 1$ because one token is removed at each step. This theoretical analysis ignores the effect of query and budget constraints on worst-case complexity. We consider these added complications in the following section.

\subsection{Empirical costs of transition-based compression}\label{s:empiricalcost}

The theoretical worst-case is a poor representation of the empirical costs of our iterative deletion algorithm. In the worst-case, iterative deletion only prunes one vertex from a singleton subtree at each timestep. But in practice, the method often prunes large subtrees which remove many vertexes in a single step. 

%Similarly, in the worst-case, the algorithm must shorten a sentence to match an arbitrarily small character budget, but in practice character budgets will be set to reasonable lengths for shortened English sentences. Finally, in the worst case, $q$ will be empty (i.e. no lexical constraints are specified, so all operations must be considered at each timestep), but in practice practitioners using our query-focused method will likely specify at least one query term which must be included in shortened sentences. 

We therefore measure the empirical costs of our algorithm during the experiment described in Section \ref{s:autoeval}. We count the total number of vertexes considered for possible pruning during the actual compression process, for each sentence in the experiment. We also compute the total number of such operations during an actual, worst-case run of the algorithm. In a worst case run, only a single token is removed at each timestep (without removing query tokens) until the character budget is satisfied.\footnote{Tokens will have different character lengths. We choose the single vertex for removal at random, from among all pruneable singleton subtrees.} 

We find that while iterative deletion is polynomial in the worst case (\S\ref{s:costs}), in practice the computational cost scales linearly in the token length of the input sentence (Figure \ref{f:example}). The observed worst case obeys the theoretical upper bound of $O(|s|^2)$ operations.

\begin{figure}[htb!]
\centering
\includegraphics[width=.5\textwidth]{observed.pdf}
\caption{Mean observed operations in a worst case run vs. mean observed operations in an average run of the iterative deletion algorithm, at different sentence lengths. In the worst case (top), iterative deletion is polynomial in the token length of the input sentence (\S\ref{s:costs}). In practice (bottom), empirical costs scale linearly for constrained compression (\S\ref{s:empiricalcost}). The observed worst case observes the theoretical upper bound of $O(|s|^2)$ operations.}
\label{f:example}
\end{figure}

\subsection{Reducing computational complexity with syntax}

\ahcomment{Possibly, redo this argument but using the Fillipova/Altun dataset rather than the Brian Dillon dataset. Cut section if not enough time or space.}

In Section \ref{s:empiricalcost}, we show that the theoretical worst case performance of iterative deletion is a poor representation of the algorithm's empirical performance. In this section, we examine the substantial gap between the mathematical description of the iterative deletion algorithm and the linguistic properties of English syntax. While in principle any vertex may be pruned from a syntax tree, in practice coherent english sentences will require verbs and subjects. Similarly, while in principle any collection of subtrees is a plausible compression, in practice coordinated English phrases must be joined with a conjunction, and prepositions (almost always) cannot be removed from the start of an English prepositional phrase. \ahcomment{look at some deps which are never pruned in FA}

\section{Related work}

Traditional study of sentence compression is closely aligned with text summarization techniques that create synopses by selecting and then shortening sentences \cite{Knight2000StatisticsBasedS,vanderwende2007beyond,clarke2008global,martins2009summarization,Nenkova2012ASO}. 
In these settings, it is important for compressions to retain ``important'' information from source sentences because they must stand-in for longer sentences within summaries.

Our concern with lexical constraints is better suited to applications in which user queries define important information in documents. For instance, our length and lexically-constrained compressions could be used in information retrieval systems that summarize search results using query-biased snippets on a search engine results page \cite{tombros1998advantages,Metzler2008MachineLS}. Often, such snippets must contain query terms, as in Figure \ref{f:qf}. 

Apache Lucene 7.5.0, the leading open source search engine, does not perform sentence compression in generating snippets using its default Fragmenter, Highlighter and Scorer modules.\footnote{https://lucene.apache.org/core/documentation.html} We plan to apply and adapt our work for use in such systems; avoiding grammatical disfluencies from naive snippet generation techniques could make search engine results more readable \cite{kanungo2009predicting}.

Our method could also be used for particular forms of query-focused summarization, such as summarizing people \cite{w04} or companies \cite{filippova2009company} which might require a hard lexical constraint.  Length and lexically constrained compressions could also be used as part of new forms of search user interfaces \cite{hearst2009search}, such as concept map browsers \cite{falke2017graphdocexplore}. 

Finally, we note that our compression method is a strictly extractive technique. We follow a line of research \cite{clarke2008global,filippova2008dependency,filippova2015sentence} in which compressions are formed from subsequences extracted from the original sentence. Other research on sentence compression uses abstractive methods to which compress sentences via paraphrasing \cite{cohn2008sentence,rush2015neural,mallinson18}. Some research in machine translation and sentence generation considers how to create paraphrased token sequences with lexical constraints \cite{N18-1119,aaimh}. 

Our method might be extended with paraphrase operations in future work, following other approaches which blend abstractive and extractive summarization \cite{P17-1099}. However, in our opinion, deploying such abstractive compression is a far-off goal. We suspect that users will not trust an abstractive summarizer which commits even slight semantic or syntactic errors \cite{Handler17,Zhang}.

\section{Conclusion and Future Work}

This work introduces a novel, neural, transition-based method for extractive sentence compression, in the spirit of early approaches to the compression task which also modified syntax trees \cite{Jing2000SentenceRF,Knight2000StatisticsBasedS}. We show that this approach is both more computationally efficient than ILP-based methods, and better reconstructs known good sentence shortenings. 

Our two principle operations, \textsc{Prune} and \textsc{Insert} can reconstruct all gold compressions in a standard compression dataset \cite{filippova2013overcoming}. However, we suspect that other operation sets can also reconstruct standard gold compressions. In particular, our method proceeds top-down, adding and removing subtrees by visiting the vertexes from the original sentence in breadth-first order. But it is also possible to imagine a bottom-up compression system which creates compressed sentences by building up compressions, starting from the leaves of the original sentence and proceeding to the root. We suspect that this bottom-up method might be more computationally efficient than our top-down approach, because at each timestep it requires a smaller number of local decisions about adding (or not adding) vertexes which are connected to the current compression in the original parse tree. We look forward to examining bottom-up compression in future work. 

Finally, our method performs sequential decision making by learning from oracle path operations at training time. When we use this oracle guidance to compress sentences, our transition-based compressor inevitably makes mistakes and diverges from the oracle path, limiting the usefulness of training data. We might apply existing research on similar sequential decision problems with oracle guidance to improve our transition-based compressor \cite{Ross2011ARO}. 

\section{Appendix}

In this work, we reimplement the method of \citet{filippova2013overcoming}, who in turn implement transformations described in \citet{filippova2008dependency}. There are inevitable discrepancies between implementations. 

\begin{figure*}[htb!]
\centering
\includegraphics[width=.75\textwidth]{worked.pdf}
\caption{Nine operations of our transition-based compression return the compression: ``Jack went up the hill". At each timestep, the compression has state $(V, v|B)$. In the diagram, the tokens in $V$ are shown in bold. At each timestep, the token $v$ at the head of the buffer $B$ is shown in the third column. Our compression system references the original, unchanging dependency parse of the sentence, $s$, shown in the diagram. The buffer is initialized breadth-first. Compression stops when the buffer is empty.}
\label{f:example}
\end{figure*}

Some differences arise from differences in syntactic formalisms. To begin, prior work uses a tree transformation method which is no longer strictly compatible with UD. For instance, the tree transformation from prior work assumes PPs are headed by prepositions, which is not true in UD. In implementing the ILP, we use the enhanced dependencies representation from CoreNLP \cite{Schuster2016EnhancedEU}. The augmented modifiers and augmented conjuncts in this representation create parses that are very similar to the transformed trees described in \citet{filippova2008dependency}. Prior also describes a syntactic constraint based on the \rdep{sc} relation, which is not included in UD. We therefore exclude this constraint.

Other differences between arise from diverging output from part-of-speech taggers. Prior work modifies a dependency tree by adding an edge between the root note and all verbs in a sentence, as a preprocessing step. This ensures that subclauses can be removed from parse trees to form compressions. However, we found that replicating this transform made it impossible for the ILP to create some gold compressions in the \citet{filippova2013overcoming} dataset; likely because different part-of-speech taggers disagree on some verb tags. We therefore add an edge between the root node and \textit{all} tokens in a sentence; we confirm that with this change the ILP can always output the gold compression.

Prior work does not specify all features in the structured perceptron model. We implement every feature discussed in the published work, except in cases where the feature or constraint assumes a syntactic formalism which is not compatible with UD.

\bibliography{abe}
\bibliographystyle{acl_natbib}

\end{document}

%\ahcomment{Sort of hard to tell what formalism F and A use. I thnk it is stanford, but they don't come out and say it.They cite Nivre's book which references the malt parser which seems to use stanford deps. but I don't see mention of the ``in" relation referenced in F and A paper in a guide to stanford deps. Writing around it.}

% other ideas... 
 
%\section{Computational experiments part 2: investigating properties of q,s,r compression}

%\ahcomment{include?}

%\ahcomment{only outline / notes here}

%\begin{enumerate}
%\item{q = a list of 1 to 3 NER}
%\item{r = random}
%\item{What is the size of the minimum compression?}
%\item{Reachability by budget by position of q in syntax tree. (If q is more than one entity then how the entities are dispersed across the tree probably matters a bunch too).}%
%\item{Hang on. reachability == min compression, eh? if min compression $>$ b, it is clearly bad.}
%\item{Avoid computational waste w/ grammar.  Examine: ops you never have to worry about if you prune a branch v. dependency type deletion endorsement rate. Some ops get rid of lots of tokens w/ very high probability of deletion endorsements: e.g. parataxis (a great op!). By contrast: pruning a noun subj destroys acceptability and usually does not delete many tokens. Not worth the risk!}
%\item{What is the empirical number of ops (i.e. decisions you have to make about pruning) if you greedily drop branches but never drop if the single op probability is less than $p$? My guess is you can make this problem way, way, simpler than implied by exponential formulation. Is it really quadratic?}
%\item{Distribution of number of ops used for different q and r: when choosing ops at random? when choosing greedily? When pruning $\propto$ p(endorsement)?}
%\item{Other stuff: min compression, reachability, operations saved w/ big prunes? position of query in the tree?}
%\end{enumerate}
