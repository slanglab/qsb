%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\newcommand{\rdep}[1]{\ $\xrightarrow{\text{\tiny #1}}$\ }
\newcommand{\speedup}[0]{4X~}
\newcommand{\ahcomment}[1]{\textcolor{blue}{[#1 -AH]}}
\newcommand{\ilptest}[0]{0.76~}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\hypersetup{draft} % THIS IS NEEDED TO GET IT TO COMPILE. Does not like the tables. AH 11/27

\newcommand\BibTeX{B{\sc ib}\TeX}

\DeclareMathOperator*{\argmaxA}{arg\,max} % Jan Hlavacek

% Linear-time Sentence Compression under Lexical and Length Constraints
% I think we should call our method query-focused because it sells why you want this in the first place. We are similar enuf to query-focused summairzation I think ths is fine.
\title{Query-focused Sentence Compression in Linear Time}

\author{Abram Handler {\normalfont and} Brendan O'Connor \\
  College of Information and Computer Sciences \\
  University of Massachusetts, Amherst \\
  {\tt ahandler@cs.umass.edu}  \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Search applications often display shortened sentences which must contain certain query terms and must fit within the space constraints of a user interface. This work introduces a new transition-based sentence compression technique developed for such settings. Our method constructs length and lexically constrained compressions in linear time, by growing a forest in the dependency parse of a sentence. This approach achieves a \speedup speed up over baseline ILP compression techniques, and better reconstructs gold shortenings under constraints. Such latency gains permit constrained compression of multiple sentences, without unreasonable lag.
\end{abstract}


\section{Introduction}\label{s:intro}

Traditional study of extractive sentence compression seeks to create short, readable, single-sentence summaries which retain the most ``important'' information from source sentences. But search applications often require compressions which must include a user's query terms and must not exceed some maximum length, permitted by screen space.  Figure \ref{f:qf} shows an example.

This study examines the English-language compression problem with such length and lexical requirements. In our constrained compression setting, a source sentence $S$ is shortened to a compression $C$ which (1) must include all tokens in a set of query terms $Q$ and (2) must be no longer than a maximum character length, $b \in \mathbb{Z}^{+}$. Formally, constrained compression maps $(S,Q,b) \rightarrow C$, such that $C$ respects $Q$ and $b$. We describe this task as query-focused compression because $Q$ places a hard requirement on words from $S$ which must be included in $C$.

\begin{figure}[htb!]
\includegraphics[width=8.5cm]{qf.pdf}
\caption{A search interface (boxed, top) returns a headline $h$ above a constrained compression $C$, shortened from a sentence $S$ in a retrieved document (italics, bottom). The constrained compression must contain all tokens in $Q$, the set of query terms (bold), and must not exceed $b=$75 characters in length.}
\label{f:qf}
\end{figure}

Existing compression techniques are poorly suited to this task. While older methods based on integer linear programming (ILP) can trivially accommodate such length and lexical restrictions \cite{clarke2008global,filippova2013overcoming}, these approaches rely on slow third-party solvers to optimize an NP-hard integer linear programming objective \cite{clarke2008global}, requiring user wait time. Meanwhile, newer LSTM tagging methods \cite{filippova2015sentence} do not allow practitioners to specify length or lexical constraints, and require expensive graphics processing units (GPUs) to achieve low runtime latency. These deficits prevent application of existing compression techniques in traditional search engines \cite{hearst2009search}, concept map browsers \cite{falke2017graphdocexplore} and new forms of exploratory textual interfaces \cite{marchionini2006exploratory}, where length, lexical and latency requirements are paramount. 

Thus, in this work, we present a new linear method for query-focused compression, which grows a forest in a sentence's dependency parse by adding vertexes one by one. We compare this \textsc{vertex addition} approach to a supervised \textsc{ilp} technique which also accommodates length and lexical restrictions. Our method has lower theoretical and empirical computational costs, while better reconstructing known-good shortenings. We release our implementation and experiments.


\begin{table}[htb!]
\begin{tabular}{lcc}
Approach & Complexity & Constrained  \\ \hline
\textsc{ilp}       &   exponential    & yes     \\
LSTM tagger & linear              & no         \\   
\textsc{vertex addition} & \textbf{linear}     &      \textbf{yes}   
\end{tabular}
\caption{Our \textsc{vertex addition} technique (\S\ref{s:system}) constructs constrained compressions in linear time. Prior work (\S\ref{s:relatedwork}) has higher computational complexity (\textsc{ILP}) or does not respect hard constraints (LSTM tagger).} 
\label{t:algos}
\end{table}

\section{Related work}\label{s:relatedwork}

Extractive compression shortens a sentence by removing tokens, typically for summarization \cite{Knight2000StatisticsBasedS,clarke2008global,P13-1136,almeida2013fast,filippova2015sentence,P16-1188,Wang2017CanSH}.\footnote{Some methods shorten sentences via generation instead of deletion \cite{rush2015neural,mallinson18}. Our interest in the extractive setting follows from a motivation to create interpretable,  trustworthy, and practical search systems \cite{Chuang2012InterpretationAT}. Users might not trust abstractive summaries \cite{Zhang:2018:MSG:3290265.3274465}, particularly in cases with semantic error.} To our knowledge, this work is the first to consider extractive compression under hard length and lexical constraints.\footnote{\citet{Li2013DocumentSV} solicit annotations for ``guided'' compression, but do not examine the compression problem under lexical and length constraints.} Our work is motivated by search user interfaces (e.g.\ Figure \ref{f:qf}), which might use such restricted shortenings to create query-biased snippets \cite{tombros1998advantages,hearst2009search}.\footnote{Apache Lucene {\small (v7.7)} does not compress sentences to form snippets \cite{lucene}.} 

We compare our \textsc{vertex addition} approach to ILP-based compression methods \cite{clarke2008global,filippova2013overcoming,Wang2017CanSH}, which can easily accommodate lexical and budget restrictions via additional optimization constraints. \textsc{ilp} methods shorten sentences using an integer linear programming objective, requiring worst-case exponential computation.\footnote{ILPs are exponential in $|V|$ when selecting tokens \cite{clarke2008global}, and exponential in $|E|$ when selecting edges \cite{filippova2013overcoming}.} 

Finally, compression methods based on LSTM taggers \cite{filippova2015sentence} cannot currently enforce lexical or length requirements. Future work might address this limitation by applying and modifying constrained generation techniques \cite{D16-1140,N18-1119,D18-1443}.

\section{Compression via \textsc{vertex addition}}\label{s:system}

We present a new transition-based method for shortening sentences under lexical and length constraints, inspired by similar approaches in transition-based parsing \cite{nivre2003}. We describe our technique as \textsc{vertex addition} because it constructs a shortening by \textit{growing} a forest in the dependency parse of a sentence, one vertex at a time. This approach differs from some prior work which
shortens sentences by \textit{pruning} subtrees \cite{Knight2000StatisticsBasedS,berg2011jointly,almeida2013fast,Filippova2015FastKS}. This additive method can construct constrained compressions in linear rather than exponential time, leading to \speedup lower latency than ILP techniques (\S\ref{s:autoeval}). Our presentation of \textsc{vertex addition} assumes a boolean relevance model: $S$ must contain $Q$. 

\subsection{Formal description}\label{s:formal}

\textsc{vertex addition} builds a compression by maintaining a state
$(C_i,P_i)$ where $C_i \subseteq S$ is a set of added candidates, $P_i  \subseteq S$ is a priority queue of vertexes, and $i$ indexes a timestep during compression. Figure \ref{f:walkthru} shows a step-by-step example. 

During initialization, we set $C_0 \gets Q$ and $P_0 \gets S \setminus Q$. Then, at each timestep, we pop some candidate $v_i =h(P)$ from the head of $P$ and evaluate $v_i$ for inclusion in $C_i$. (Neighbors of $C_i$ in $P$ get higher priority than non-neighbors; we break ties in left-to-right order, by sentence position.) If we accept $v_i$, then $C_i \gets C_i \cup v_i$. We discuss acceptance decisions in detail in \S\ref{s:transition}. We continue adding vertexes to $C$ until either $P$ is empty or $C_i$ is $b$ characters long.\footnote{We linearize $C$ by left-to-right vertex position in $S$, which is common for English-language compression.} The appendix includes a formal algorithm. 

\textsc{vertex addition} is linear in the token length of $S$ because we pop and evaluate some vertex from $P$ at each timestep, after $P_0  \gets S \setminus Q$. Additionally, because (1) we never accept $v_i$ if the length of $C_i \cup v_i$ is more than $b$, and (2) we set $C_0 \gets Q$, our method respects $Q$ and $b$.

\begin{figure}[h]
\includegraphics[width=8.2cm]{additive.pdf}
\caption{A dependency parse of a sentence $S$, shown across five timesteps of \textsc{vertex addition} (from left to right). Each node in the parse is a vertex in $S$. Our stateful method produces the final compression $\{$A,C,B,E$\}$ (rightmost). At each timestep, each candidate $v_i$ is boxed; rejected candidates $\neg C_i$ are unshaded.}
\label{f:walkthru}
\end{figure}

\section{Evaluation}\label{s:autoeval}

We observe the latency, readability and token-level F1 score (the standard automatic metric in this setting)  of our \textsc{vertex addition} method in a constrained compression experiment, using a standard dataset (\S\ref{s:constrained}). We compare to a leading, supervised \textsc{ilp} baseline (\S\ref{s:relatedwork}) because ILP methods are the only known technique for constrained compression. Each method shortens to the same compression rate, following best evaluation practices \cite{napoles2011evaluating}. \textsc{vertex addition} achieves higher F1 and readability scores (Table \ref{t:results}), with \speedup lower latency  than the \textsc{ilp}. We evaluate the significance of each difference with bootstrap sampling \cite{D12-1091}. All differences are significant {\small $(p < 10^{-2})$}. 

\subsection{Constrained compression experiment}\label{s:constrained}

In order to evaluate different approaches to constrained compression, we require a dataset of sentences, constraints and known-good shortenings, which respect the constraints. This means we need tuples $(S, Q, b, C_g)$, where $C_g$ is a known-good compression of $S$ which respects $Q$ and $b$ (\S\ref{s:intro}).

To support large-scale automatic evaluation, we reinterpret a standard compression corpus \cite{filippova2013overcoming}
as a collection of input triples and constrained compressions. The original dataset contains pairs of sentences $S$ and compressions $C_g$, generated using headlines. For our experiment, we set $b$ equal to the character length of the gold compression $C_g$. We then sample some query set $Q$ from the proper and improper nouns in $C_g$ so that the distribution of cardinalities of queries across our dataset simulates the observed distribution of cardinalities (i.e.\ number of query tokens) in real-world search \cite{Jansen2000RealLR}. Sampled queries are short sets of nouns, such as ``police, Syracuse'', ``NHS'' and ``Hughes, manager, QPR,'' approximating real-world behavior \cite{Barr2008TheLS}.\footnote{See appendix for detailed discussion of query sampling.} 

By sampling queries and defining budgets in this manner, we create {199,152} training tuples and {9,969} test tuples, each of the form $(S,Q,b,C_g)$. \citet{filippova2013overcoming} define the train/test split. We re-tokenize, parse and tag with CoreNLP v3.8.0 \cite{corenlp}. We reserve 24,999 training tuples as a validation set.

The appendix describes additional details of our latency and readability experiments.

\subsection{Implementation: \textsc{ilp}}\label{s:ilp}

We compare our system to a baseline, \textsc{ilp} method, presented in \citet{filippova2013overcoming}. This approach represents each edge in a syntax tree with a vector of real-valued features, then learns feature weights using a structured perceptron trained on a corpus of $(S,C_g)$ pairs. Learned weights are used to compute a global compression objective, subject to structural constraints which ensure $C$ is a valid tree. This baseline can easily perform constrained compression: at test time, we add optimization constraints specifying that $C$ must include $Q$, and not exceed length $b$.

To our knowledge, a public implementation of this method does not exist. We reimplement from scratch using \citet{gurobi}, achieving a test-time, token-level F1 score of \ilptest on the unconstrained compression task, lower than the result {\small (F1 = 84.3)} reported by the original authors. There are some important differences between our reimplementation and the method reported in \citet{filippova2013overcoming}, largely resulting from differences in syntactic formalisms. We describe these differences in detail in the appendix. Note that our additive method requires $Q$ and $b$, so we can only compare it to the ILP on the \textit{constrained} (rather than traditional, unconstrained) compression task.

\subsection{Implementation: \textsc{vertex addition}}\label{s:transition}

Vertex addition accepts or rejects some candidate vertex $v_i$ at each timestep $i$. 
We learn such decisions using a corpus of tuples $(S,Q,b,C_g)$ (\S\ref{s:constrained}). Given such a tuple we can construct a unique oracle compression path by $(1)$ initializing $C_0$ with $Q$, $(2)$ choosing $v_i = h(P)$ at each timestep, and $(3)$ adding $v_i$ to $C_i$ iff $v_i \in C_g$. This procedure can reconstruct all $C_g$ in the original \citet{filippova2013overcoming} corpus. 

We use such oracle paths to train a model of oracle inclusion decisions, ${p(y_i  = 1 | v_i, C_i, S)}$. We initially experimented with neural techniques, following the \citet{D14-1082} approach to transition-based parsing. But we found that feature-based, binary logistic regression achieved similar performance with much lower latency. This non-neural approach is also better suited to applications in fields like social science and journalism, where expensive GPUs are not available. The features in our model fall into 3 classes.

\textbf{Edge features} describe the properties of the edge $(u,v_i)$ between $v_i \in P$ and $u \in C_i$. We use the edge-based feature function from \citet{filippova2013overcoming}, described in detail in the appendix. This allows us to compare the performance of a vertex addition method based on local decisions with an ILP method that optimizes a global objective (\S \ref{s:ablated}), using the same feature set.

\textbf{Stateful features} represent the relationship between $v_i$ and the compression $C_i$ at timestep $i$. Stateful features include information such as the position of $v_i$ in the sentence, relative to the right-most and left-most vertex in $C_i$, as well as history-based information such as the fraction of the character budget used so far. Such features allow the model to reason about which sort of $v_i$ should be added, given $Q$, $S$ and $C_i$.

\textbf{Interaction features} are formed by crossing all stateful features with the type of the dependency edge governing $v_i$, as well as with indicators identifying if $u$ governs $v_i$, if $v_i$ governs $u$ or if there is no edge $(u,v_i)$ in the parse.

We implement with Python 3 using scikit-learn \cite{Pedregosa:2011:SML:1953048.2078195}. We tune the inverse regularization constant to $c=10$ via grid search over powers of ten, to optimize validation set F1. 

\subsection{Analysis:  \textsc{ablated} \& \textsc{random}}\label{s:ablated}
For comparison, we implement an \textsc{ablated} vertex addition method, which makes inclusion decisions using a model trained on only edge features from \citet{filippova2013overcoming}. This method achieves a lower F1 than the ILP, which integrates the same edge-level information to optimize a global objective. However, adding stateful and interaction features (Table \ref{t:results}, bold) to inform local vertex addition decisions raises F1 score. The relatively strong performance of \textsc{ablated} hints that edge-level information alone can largely guide $v_i$ decisions, e.g.\ should some verb $v_i$ governing some object in $C_i$ via $(v_i,u)$ be included?

We also evaluate a \textsc{random} baseline, which accepts each $v_i$ randomly in proportion to $p(y_i = 1)$ across training data. \textsc{random} achieves reasonable F1 because (1) $C_0 = Q \in C_g$ and (2) F1 correlates with compression rate \cite{napoles2011evaluating}, and $b$ is set to the length of $C_g$.

\subsection{Importance and readability evaluation}\label{s:readabilityinformativeness}

Researchers often use human judgements of \textit{importance} and \textit{readability} to evaluate compression techniques \cite{Knight2000StatisticsBasedS,filippova2015sentence}. In our setting $Q$ determines the ``important'' information from $S$, so human importance evaluations are inappropriate.

We use the automated SLOR metric \cite{lau2015unsupervised} to check the readability of compressions. SLOR normalizes the probability of a token sequence assigned from a language model, by adjusting for both the probability of the individual unigrams in the sentence and for the sentence length.\footnote{Longer sentences are always less probable than shorter sentences; rarer words make a sequence less probable.} SLOR is known to correlate with human judgements of compression readability \cite{kannConl}.  

\subsection{Latency evaluation}\label{s:costs}

\textsc{vertex addition} has lower theoretical complexity than the \textsc{ilp} baseline. We test corresponding empirical latency gains by measuring the clock speed of each compression method, in milliseconds per sentence. \textsc{vertex addition} is on average \speedup faster than the \textsc{ilp} (Table \ref{t:results}). Added speedups might be possible by reimplementing our Python 3 code in a more performant language such as C.

In search applications, latency gains are non-trivial: real users are measurably hindered by interface lags \cite{Nielsen,heerschei,Liu2014TheEO}.
 %Unlike ILP-based methods (which rely on black-box solvers), our Python 3 implementation could also be sped up by reimplementing in faster languages like Java or C. 

\begin{table}[]
\begin{tabular}{lccc}
\centering
Approach & F1 & SLOR &  ms. / sentence  \\ \hline
\textsc{random} {\small (lower bound) }&{\small 0.653}&{\small 0.678}&{\small 0.5}\\
\textsc{ilp}&{\small 0.864}&{\small 1.141}&{\small 21.9}\\
\textsc{ablated} {\small (edge only) }&{\small 0.820}&{\small 1.062}&{\small 3.5}\\
\textbf{\textsc{vertex addition}}&\textbf{\small 0.884}&\textbf{\small 1.149}&\textbf{\small 5.2}\\
\end{tabular}
\caption{Validation results for constrained compression. All differences are significant {\small $(p < 10^{-2})$}. Latency is shown in milliseconds (ms.)}
\label{t:results}
\end{table}

\section{Conclusion}

Both novel and traditional search interfaces would benefit from low-latency, query-focused and length-constrained compression. We introduce a new \textsc{vertex addition} technique for such settings, which is \speedup faster than an ILP baseline while achieving higher F1 and readability scores. 

%Finally, some shortened sentences will modify the meaning of a sentence. Identifying such cases is a special case of the unsolved textual entailment problem \cite{snli_bowman,Pavlick2016SoCalledNA,linzencompression}. In the future, we plan to apply entailment research to the compression task.  

% ack => Katie, Javier, Nick Eubank, NLP reading group! 

%\include{appendix_content}

\bibliography{abe}
\bibliographystyle{acl_natbib}

\end{document}