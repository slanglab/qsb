%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\newcommand{\rdep}[1]{\ $\xrightarrow{\text{\tiny #1}}$\ }
\newcommand{\ahcomment}[1]{\textcolor{blue}{[#1 -AH]}}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\hypersetup{draft} % THIS IS NEEDED TO GET IT TO COMPILE. Does not like the tables. AH 11/27

\newcommand\BibTeX{B{\sc ib}\TeX}

\DeclareMathOperator*{\argmaxA}{arg\,max} % Jan Hlavacek

\title{Extractive sentence compression under lexical and length constraints}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Traditional approaches to extractive sentence compression seek to reduce the length of a sentence, while retaining the most ``important'' information from the source. But query-focused applications such as document search engines or exploratory search interfaces place additional lexical and length requirements on compression systems. This study introduces a new, linguistically-informed compression method which can accommodate such requirements.  Our method is more computationally efficient than classical ILP-based approaches, and more accurately reconstructs known-good shortenings.
\end{abstract}

\section{General comment}
Nobody cares about Rookie. Make the argument for open source search engines. 

\ahcomment{TODO: Brendan's email}

\section{Unknowns}

\ahcomment{How will we deal with semantics? Brendan email about subsective adjectives ellie pavlik work. Right now we just use the gold data for this I guess. Future work?}

\ahcomment{How to use data from Brian, if at all. It uses dep type info + LM info + worker info. We use dep type info already. Can we use LM info?}

\section{Introduction}

Traditional study of extractive sentence compression seeks to create short, readable compressions which retain the most ``important'' information from source sentences. But query-focused and user-facing applications impose additional requirements on the output of a compression system. Compressions must be short enough to be shown in a user interface; and often must contain a user's query term. An example of such a compression is shown in Figure \ref{f:qf}.

This study examines the compression problem with length and lexical constraints: shortened sentences must meet a given character budget, and must include particular words. While older compression methods based on integer linear programming could trivially accommodate such restrictions, recent work has focused on neural network techniques \cite{filippova2015sentence} which do not give practitioners such control. This makes existing neural methods unsuitable for search engines \cite{hearst2009search}, concept map browsers \cite{falke2017graphdocexplore} and new forms of exploratory textual interfaces \cite{marchionini2006exploratory}, where length and lexical constraints are paramount. 

\begin{figure}[htb!]
\includegraphics[width=9cm]{qf.pdf}
\caption{Search interfaces often require compressions with length and lexical constraints. In this case, the user has queried for ``Pemex contracts"; a system returns a related link and a short compression which contains the query terms}
\label{f:qf}
\end{figure}


Therefore, in this work:

\begin{itemize}
\item{We present a new, linguistically-informed method for compressing sentences, which leverages the large corpora used to train neural networks, while also accommodating lexical and length constraints.}
\item{Compare our transition-based technique to traditional ILP-based methods, which also give a user such control.}
\item{We analyze the computational costs of each technique, as well as the nature of the constrained compression problem itself}
\end{itemize}


\section{Compression and constraints}

This work discusses contrasts traditional ILP-based compression with our novel transition-based technique. We also briefly discuss compression with LSTM taggers, which do not accommodate length or lexical requirements.

\begin{table*}[htb!]
\begin{tabular}{cccc}
\textbf{Approach} & \textbf{Worst-case complexity} & \textbf{Constrained} & \textbf{Allows supervision} \\ \hline
LSTM taggers      & linear              & no     &    yes      \\   
linear programming              & exponential         & yes    &    yes   \\
prune, predict, loop  (this work)    & quadratic*           & yes    &      yes   \\
predict, then prune  (this work)    & linear           & yes    &      yes   \\
\end{tabular}
\caption{\ahcomment{TODO}}
\end{table*}

\subsection{Constrained compression with ILPs}

One common approach for shortening sentences formulates compression as an integer linear programming (ILP) task. ILP-based methods assign binary variables to each token in a sentence \cite{clarke2008global} or subtree in a dependency parse \cite{filippova2008dependency}. These variables indicate if the corresponding component included in a compression. Each such sentence component is also assigned a local weight, indicating its worthiness for inclusion in a shortened sentence. Local weights are either learned with structured perceptron techniques \cite{filippova2013overcoming}, or inferred from corpus statistics \cite{filippova2008dependency}, word importance scores and language models \cite{clarke2008global}.

ILP methods represent the overall quality of a compression by summing the local weights of sentence components to compute a global objective score.  The problem of identifying the best possible solution to an integer programming compression objective is exponential in the number of tokens or number of subtrees in the input, as each binary variable may be set to zero or one. Researchers use off-the-shelf ILP solvers to identify the highest-scoring compression, from among the exponential possible configurations of binary variables.

This integer linear programming approach also easily accommodates constrained compression. When performing ILP-based compression, researchers will customarily add constraint terms to the ILP objective in order to preserve the meaning of a sentence (e.g. don't remove negations), or to ensure that output forms a valid tree (e.g. each subtree must have a parent vertex).

For these methods, adding additional length or lexical requirements is trivial: practitioners must specify that optimal solutions must be shorter than some character budget and must specify that binary variables marking inclusion of particular words must be set to 1. 

\subsection{Transition-based constrained compression}

In this work, we present an alternative, transition-based method for shortening sentences under lexical and length constraints \cite{Earley1970AnEC,nivre2003}, inspired by transition-based parsing techniques. Our method compresses a sentence over $N$ time steps, by adding and removing $N$ different subtrees from a dependency parse, one after another. This method recalls early solutions to the compression problem \cite{Jing2000SentenceRF,Knight2000StatisticsBasedS}, which also shortened sentences by executing grammatically-motivated operations on syntactic representations of trees. We present the formal details in \S\ref{s:system}.

Like ILP-based methods, transition-based approaches can easily accommodate lexical and length restrictions. At a high-level, such methods need to add subtrees which contain query terms and remove subtrees which do not contain subtrees, until identifying a compression which satisfies the length constraints. In this work, we show that transition-based methods incur much smaller computational costs than integer programming techniques, while achieving higher token-level F1 scores than ILP-based compression.

\subsection{Unconstrained compression with LSTMs}

We contrast transition-based compression and integer programming approaches with LSTM taggers for the compression task \cite{filippova2015sentence}. Such taggers achieve state-of-the-art scores on extractive sentence compression, using sequence-to-sequence models that label tokens with a 1 or a 0 indicating if the token should be included in a summary. This approach is linear in the token length of a the input sequence. However, at this time, LSTM taggers are unsuitable for query-focused applications because such methods cannot enforce lexical or length requirements. This limitation might be examined in future research.

\section{Transition-based sentence compression}\label{s:system}

In this work, we present a new transition-based sentence compression system. We formally describe the system (\S\ref{s:formal}), present a method for generating oracle paths to gold standard shortenings (\S\ref{s:oracle}), and then detail a computational model of oracle compression (\S\ref{s:modeling}).

\subsection{Formal description}\label{s:formal}

Our transition-based sentence compression system shortens a sentence $s$ by executing a sequence of operations. Each operation modifies the state, denoted $(V,v|B)$, where $V$ is a subset of tokens from $s$, and $B$ is an ordered buffer of tokens from $s$ headed by the token $v$. In defining each operation, we use the notation $T(v)$ to denote the subtree headed by $v$ in the unchanging, precomputed dependency parse of the original sentence.

We define four principal operations, which reference the vertex $v$ at the head of the buffer $v|B$. The operation \textbf{\textsc{Prune}} removes all vertexes in $T(v)$ from $V$. The operation \textbf{\textsc{Insert}} adds $T(v)$ into $V$. \textsc{Prune} and \textsc{Insert} each remove $v$ from $B$. The compression system can also execute the operations \textbf{\textsc{NoPrune}} and \textbf{\textsc{NoInsert}}, which remove $v$ from $B$ without inserting or pruning the subtree $T(v)$. 

To generate a compression, we execute the $\textsc{Start}$ operation which initializes $V$ to be an empty set and initialize $B$ to be the tokens in $s$, arranged breadth-first. After the buffer is empty, we execute a $\textsc{Stop}$ operation; all tokens in $V$ are then linearized in their original order to return a shortened sentence. The appendix includes a complete, worked example. Table \ref{t:ops} defines all operations formally. 

In our transition-based compressor, only some operations are valid for some configurations of the state. If $v \notin V$, then $\textsc{Prune}$ is not valid as $T(v)$ cannot be removed from $V$. Similarly, if $v \in V$, then $\textsc{Insert}$ is not valid as $T(v)$ cannot be added to $V$. Table \ref{t:ops} includes these preconditions.

\subsection{Oracle paths}\label{s:oracle}

We identified the operations in our compression system via empirical experiment: we found that for all compressions in a large, standard corpus \cite{filippova2013overcoming} there exists an oracle path of at most $|s|$ operations which can fully reconstruct the shortened sentence. $|s|$ denotes the token length of the original sentence.

We can assign an oracle operation to each $v$ in $s$ by walking the dependency parse of the original sentence breadth-first, and identifying which operation is consistent with the gold-standard compression. For instance, if some vertex $v$ is not present in the gold compression, but is present in ${V}$ at a given timestep, then we say that the oracle operation for $v$ must be \textsc{Prune}. Similarly, if $v$ is present in the gold compression, but is not present in ${V}$ at a given timestep, we say that the oracle operation for $v$ must be \textsc{Insert}.\footnote{Because we walk the dependency parse exactly once in breadth-first fashion to generate gold compressions, each operation in the oracle path affects increasingly nested subtrees. Thus, if a vertex $v$ is or is not included in a compression it must be pruned or inserted when it is at the head of the buffer.} For roughly two-thirds of vertexes in the breath-first walk, the oracle executes $\textsc{NoPrune}$ or $\textsc{NoInsert}$ which removes $v$ from $B$ without modifying $V$.

Because only \textsc{Prune} or \textsc{Insert} is valid for a given state (\S\ref{s:formal}), reconstructing a gold compression can be interpreted as a sequence of binary decisions. If $v \in V$, the compression system must decide to execute $\textsc{Prune}$ or $\textsc{NoPrune}$. If $v \notin V$, it must decide to execute $\textsc{Insert}$ or $\textsc{NoInsert}$. The compression system must make a binary decision for any given state--vertex pair.

\begin{figure*}[htb!]
\centering
\includegraphics[width=.75\textwidth]{example.pdf}
\caption{\ahcomment{TODO fix} }
\label{f:example}
\end{figure*}

\subsection{Modeling}\label{s:modeling}

Loosely following \citet{D14-1082}, we define a set of tuples $\{(\bm{x}_j, y_j) \}_{j=1}^{K}$, where $\bm{x}$ is a sequence of symbols reflecting some configuration of the compression system at some step $j$ on some oracle path and $y_j \in \{0,1\}$ is a binary variable indicating the oracle choice for that configuration. The appropriate binary choice (e.g. prune or no prune) is always clear from context (\S\ref{s:oracle}). We use a standard compression corpus \cite{filippova2013overcoming} to generate \ahcomment{TODO} tuples, which we use to train a LSTM.

\textbf{Markup function.} We define a markup function $m(V,v,o)=\bm{x}$ to generate the sequence of input symbols for the LSTM. The markup function is meant to ``show'' the model what a compressed sentence would ``look'' like if the compression system were to execute the operation $o \in \{\textsc{Prune}, \textsc{Insert}\}$. For insert operations, the markup ``shows'' what the resulting compression would be, if the subtree $T(v)$ were copied over into $V$; for prune operations, it ``shows'' what the compression would be if $T(v)$ were removed.

More concretely, $\bm{x}$ is the symbol sequence [SOS, $V_L$, $\langle {o \cdot d} \rangle$, $T(v)$, $\langle{/o\cdot d}\rangle$, $V_R$, EOS], where:

\begin{itemize}
\item{$T(v)$ denotes all vertexes in the subtree rooted at $v$ in the original sentence $s$, linearized in their original order.}
\item{The symbols SOS and EOS are special markers denoting the start and end of the sequence.}
\item{$V_L$ is all vertexes from $V$ which occur to the left of $T(v)$ in $s$, linearized in their original order. $V_R$ is identical to $V_L$, except that vertexes in $V_R$ must occur to the right of $T(v)$.}
\item{The symbol $\langle o \cdot d \rangle$ is formed by concatenating two different markers: (1) a categorical marker indicating $d$, the dependency type governing $T(v)$, and (2) a start symbol of type $o$ indicating the beginning of the sequence of tokens affected by the operation of type $o$. The symbol $\langle / o \cdot d \rangle$ is formed identically, only the marker indicates the end of the sequence.\footnote{BracketTags allow us to encode some aspects of syntax trees using a vanilla LSTM \cite{Vinyals2015GrammarAA,Aharoni2017TowardsSN}, without the added complexities of explicitly encoding nested structures into the architecture of the network \cite{Tai2015ImprovedSR,Dyer2016RecurrentNN}.}}
\end{itemize}

For instance, \ahcomment{Talk thru a whole example. Rewrite} in Figure \ref{f:example}, the BracketTag \textsc{Prune}--nmod indicates that (1) $T(v)$ is governed by an \rdep{nmod} relation, and (2) the next tokens \textsc{Prune}

\footnote{$v$ is assumed to be the token at the start of the buffer, and the buffer must be initialized breadth-first.}

\ahcomment{TODO details: class weighting optimizer, stopping criteria,  embeddings,loss function,do you update embeddings,tuning}

During training, we look up each token in the sequence in an embeddings matrix and pass these embeddings to a Bi-LSTM. Following recent work on LSTM classification for sentence-level tasks \cite{D17-1070}, we use a max pooling layer to encode a variable-length sequence into a fixed-length vector, which we pass through a sigmoid and then softmax layer to predict the oracle 
operation. 


\section{Experiment: Automatic Evaluation of Constrained Compression}

We use a large, standard dataset of sentence--compression pairs from \citet{filippova2013overcoming} to both train and evaluate different approaches to length and lexically constrained compression. Using the dataset in this manner requires reinterpreting gold standard data, which does not specify either a query or length constraint. After re-tokenizing, parsing and tagging NER spans with Stanford CoreNLP 3.8.0 \cite{corenlp}, we define all NER tokens\footnote{We use a standard 3-class definition of NER; for our purposes, entities are people, locations or organizations.} which lie within the gold compression as the query, $q$. Formally, $q$ is a list of tokens. We also define the character length of the gold-standard compression as the character budget, $b$. This interpretation allows us to redefine the corpus of $(s,c)$ pairs as a corpus of 4-tuples $(q,s,b,c)$, where $q$ is the query, $s$ is the original sentence, $b$ is the character budget and $c$ is some readable compression which satisfies the length and lexical constraints.  \ahcomment{Note: these Q and r are going to be well-behaved! I.e. there is usually a good compression here. Should be revisited.}

We then use token-level F1, to measure how well an ILP-based method (\S\ref{s:ilp}) and a transition-based method (\S\ref{s:transition}) method can reconstruct each known-good compression $c$, given $q,s$ and $b$. Token-level F1 is the standard automatic evaluation metric for the sentence compression task. We describe each method in the following sections.

\ahcomment{human eval?}
    
\begin{table}[htb!]
\begin{tabular}{ll}
\centering
Approach & F1  \\ \hline
Human acceptability supervision \ahcomment{????}        &  X        \\
F and A (2013)    & .79           \\
C and L (2008)  \ahcomment{????}  & X      \\
\textbf{Prune,predict,loop} &  \textbf{.86}    \\ 
\textbf{Transition-based (this work)} &  \textbf{.86}    \\   
\end{tabular}
\end{table}

\ahcomment{todo p-values}


\subsection{Implementation: transition-based compression}\label{s:transition}

Our model of transition-based sentence compression (\S\ref{s:modeling}) predicts the oracle operation $y$, given the state $(V_c, E_c)$. There are many possible ways to use $p(y|(V_c, E_c)$ in a query-focused and length-constrained compression system. We use a simple \textbf{iterative deletion} technique: at each step $j$ we \textsc{Prune} the subtree rooted at 

$$\argmaxA_{v \in V_c,q\not\in T(v)}   p(y_j = \textsc{Prune}(v) | (V_c, E_c)_j)$$

\noindent where $(V_c, E_c)_j$ represents the state of the compression system at time $j$. We continue pruning subtrees until the length of the linearized tokens in $V_c$ is less than $b$, in which case we stop compression. 

Because \textsc{Prune} only removes tokens from the state, we initialize $(V_c, E_c)$ with the smallest subtree of $(V_s, E_s)$ which is (1) rooted at a verb and (2) contains all of the tokens in $q$. In \ahcomment{TODO \%} of cases $(V_c, E_c)$ is simply equal to $(V_s, E_s)$. The remaining cases are instances when all tokens in $q$ are contained in some sub-clause of the sentence; the compression is formed by shortening this subclause, instead of shortening the whole sentence.

\ahcomment{hand-wavey here. Fix.}

\ahcomment{Better job evaluating this compression. How far do you get  by pruning}

\ahcomment{What is the lower bound on F1 from just guessing query tokens} 

\ahcomment{Some algos discussed at meeting w/ brendan: iterative bfs w/ lowering epsilon, predict then prune, predict,prune, loop}

\subsection{Implementation: ILP-based compression}\label{s:ilp}

We compare our transition-based compression system to the state-of-the-art ILP-based method, presented in \citet{filippova2013overcoming}. To our knowledge, the authors did not release code for this model. We reimplement from scratch, achieving a similar a token-level F1 score as the original authors \ahcomment{.72, roughly}. We use a leading off-the-shelf optimizer \cite{gurobi} to solve the ILP.\footnote{We use version 7.} 

There are some important differences between our implementation and existing work, which arise in part because we use Universal Dependencies (v1) instead of the older dependency system employed in prior work \ahcomment{See comment in appendix. talk to brendan. stanford??}. We detail these differences in the appendix. Following \citet{filippova2013overcoming}, we set the weights of the ILP with a structured perception trained on the 100,000 sentence--compression pairs from the corpus.

\ahcomment{gurobi pool search mode is turned ON which makes everythign go slower. you should turn this off if you start making arguments about wall clock time}

\ahcomment{do you train the ILP to accept query terms? that is not really a baseline. }

\ahcomment{100k in training set. we do too }

\ahcomment{how to assess convergence}

\ahcomment{implementing get Q prefers to get the root. }


\ahcomment{what do they do in IR for query biased snippets? Hamed, John, Jieppu.
 (Prior study suggests that grammatical malformations cause readability errors in search results \cite{kanungo2009predicting}).
 Find out what they do in lucene. Argue about open source search engines. }



\begin{table*}[]
\centering
\begin{tabular}{llp{70mm}}
\textbf{Operation} &             \textbf{Definition}                                                    &      \textbf{Description}    \\ \hline
\textsc{Start}      & START $\Rightarrow ( V=\emptyset,  B=[v_1, v_2 ... v_n])$ & Initialize the buffer with the vertexes in the original sentence $s$, arranged breadth-first \\ \hline
\textsc{Prune}              & $(V, [v|B]), v \in V,  \Rightarrow (V \setminus  T(v), B)$ & Remove the subtree rooted at $v$ in $s$ from $V$ \\  
$\textsc{Insert}$             & $(V, [v|B]), v \notin V, \Rightarrow (V \cup T(v), B)$ & Insert the subtree rooted at $v$ in $s$ into $V$  \\ \hline
\textsc{NoPrune}           & $(V, [v|B]), v \in V, \Rightarrow (V, B)$ & Don't remove the subtree rooted at $v$ from $V$  \\ 
\textsc{NoInsert}          &       $(V, [v|B]), v \notin V, \Rightarrow (V, B)$ &   Don't insert the subtree rooted at $v$ into $V$    \\ \hline
\textsc{Stop}             & $ (V, B=[]) \Rightarrow$ STOP & Compression ends when the buffer is empty \\                                               
\end{tabular}
\caption{A transition-based sentence compression system with a \textsc{Prune} and \textsc{Insert} operation. The state of the compression system is a tuple $(V, B)$, where $B$ is an ordered buffer of tokens and $[v|B]$ indicates that $v$ is at the head of the buffer. $V$ is a subset of vertexes from the original sentence $s$. $T(v)$ denotes the subtree rooted at $v$ in the original sentence. The \textsc{Prune} operation removes all tokens in $T(v)$ from $V$. The \textsc{Insert} operation adds all vertexes from $T(v)$ into $V$. If the compression system does not execute a \textsc{Prune} or \textsc{Insert}, $v$ is removed from the head of the buffer and $V$ is not modified. $B$ is initialized with all tokens from $s$ (arranged in breadth-first order) at the \textsc{Start} of compression. Once the buffer is empty, compression will \textsc{Stop} and all vertexes in $V$ are linearized in their original order. These operations can fully reconstruct all shortenings in a standard compression corpus \cite{filippova2013overcoming}. The appendix presents a complete, worked example.}
\label{t:ops}
\end{table*}

\section{Computational experiments part 2: investigating properties of q,s,r compression}

\ahcomment{only outline / notes here}

\begin{enumerate}
\item{q = a list of 1 to 3 NER}
\item{r = random}
\item{What is the size of the minimum compression?}
\item{Reachability by budget by position of q in syntax tree. (If q is more than one entity then how the entities are dispersed across the tree probably matters a bunch too).}
\item{Hang on. reachability == min compression, eh? if min compression $>$ b, it is clearly bad.}
\item{Avoid computational waste w/ grammar.  Examine: ops you never have to worry about if you prune a branch v. dependency type deletion endorsement rate. Some ops get rid of lots of tokens w/ very high probability of deletion endorsements: e.g. parataxis (a great op!). By contrast: pruning a noun subj destroys acceptability and usually does not delete many tokens. Not worth the risk!}
\item{What is the empirical number of ops (i.e. decisions you have to make about pruning) if you greedily drop branches but never drop if the single op probability is less than $p$? My guess is you can make this problem way, way, simpler than implied by exponential formulation. Is it really quadratic?}
\item{Distribution of number of ops used for different q and r: when choosing ops at random? when choosing greedily? When pruning $\propto$ p(endorsement)?}
\item{Other stuff: min compression, reachability, operations saved w/ big prunes? position of query in the tree?}
\end{enumerate}

\section{computational experiments part 3: computational costs}

\ahcomment{See Brendan's notes on Trapezoidal costs Nov 8th on phone during meeting}

\ahcomment{this needs to be rewritten}

In the worst case, an iterative deletion method will prune one singleton subtree (consisting of only one vertex) at each of the $N$ timesteps during compression. (Starting from the leaves and proceeding to the root of the original dependency parse). If a dependency parse contains $V$ original vertexes, and each remaining vertex is evaluated for possible deletion at each timestep, then the iterative deletion method requires at most ${\sum_{i = 0}^N V - i = O(V^2)}$ operations.

This worst case represents the theoretical upper bound of iterative deletion approaches. In practice, there is a substantial gap between the mathematical properties of the iterative deletion algorithm and the linguistic properties of English syntax. Coherent english sentences require verbs and subjects. Coordinated English phrases must be joined with a conjunction. Prepositions cannot be removed from the start of an English prepositional phrase. In section \S\ahcomment{TODO} we examine how a dataset of human judgements of the well-formedness of shortened sentences can dramatically reduce the empirical complexity of the iterative deletion algorithm by blocking obviously terrible prunes. While in principle there are an exponential number of possible sentence compressions, in practice there is a much smaller set of fluid or coherent shortened sentences. 

In section \S\ahcomment{TODO} we also examine how the order of subtree deletion affects performance: intuitively, pruning large trees early in deletion removes the need to evaluate their many subtrees later in the deletion process.



\section{Related work}

Traditional study of sentence compression is closely aligned with text summarization techniques that select and  shorten sentences \cite{Knight2000StatisticsBasedS,vanderwende2007beyond,clarke2008global,Nenkova2012ASO}. 
In these settings, it is important for compressions to retain ``important'' information from source sentences because they must stand-in for longer sentences within summaries.

Our concern with lexical constraints is better suited to applications in which user queries define important information in documents. For instance, our length and lexically-constrained compressions could be used in information retrieval systems that summarize search results using query-based snippets on a search engine results page \cite{tombros1998advantages,Metzler2008MachineLS}. Often, such snippets must contain query terms. \ahcomment{screenshot?}

Our method could also be used for particular forms of query-focused summarization, such as summarizing people \cite{w04} or companies \cite{filippova2009company} which might require a hard lexical constraint. (Other work on query-focused summarization \cite{das} assumes softer constraints). 

Length and lexically constrained compressions could also be used as part of new forms of search user interfaces \cite{hearst2009search}, such as concept map browsers \cite{falke2017graphdocexplore}. Our interest in this problem arose from constructing one such novel search system.

\section{Conclusion}
\ahcomment{TODO}

\section{Appendix}

In this work, we reimplement the method of \citet{filippova2013overcoming}, who in turn implement transformations described in \citet{filippova2008dependency}. There are inevitable discrepancies between implementations. 

Some differences arise from differences in syntactic formalisms. To begin, prior work uses a tree transformation method which is no longer strictly compatible with UD. For instance, the tree transformation from prior work assumes PPs are headed by prepositions, which is not true in UD. In implementing the ILP, we use the enhanced dependencies representation from CoreNLP \cite{Schuster2016EnhancedEU}. The augmented modifiers and augmented conjuncts in this representation create parses that are very similar to the transformed trees described in \citet{filippova2008dependency}. Prior also describes a syntactic constraint based on the \rdep{sc} relation, which is not included in UD. We therefore exclude this constraint.

Other differences between arise from diverging output from part-of-speech taggers. Prior work modifies a dependency tree by adding an edge between the root note and all verbs in a sentence, as a preprocessing step. This ensures that subclauses can be removed from parse trees to form compressions. However, we found that replicating this transform made it impossible for the ILP to create some gold compressions in the \citet{filippova2013overcoming} dataset; likely because different part-of-speech taggers disagree on some verb tags. We therefore add an edge between the root node and \textit{all} tokens in a sentence; we confirm that with this change the ILP can always output the gold compression.

Prior work does not specify all features in the structured perceptron model. We implement every feature discussed in the published work.

\ahcomment{Sort of hard to tell what formalism F and A use. I thnk it is stanford, but they don't come out and say it.They cite Nivre's book which references the malt parser which seems to use stanford deps. but I don't see mention of the ``in" relation referenced in F and A paper in a guide to stanford deps. Writing around it.}


\section{Worked example}

Jack and Jill went up the hill. 

Jill went up.

\bibliography{abe}
\bibliographystyle{acl_natbib}

\end{document}
