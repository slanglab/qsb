%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\newcommand{\rdep}[1]{\ $\xrightarrow{\text{\tiny #1}}$\ }


\newcommand{\ahcomment}[1]{\textcolor{blue}{[#1 -AH]}}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\hypersetup{draft} % THIS IS NEEDED TO GET IT TO COMPILE. Does not like the tables. AH 11/27

\newcommand\BibTeX{B{\sc ib}\TeX}

\DeclareMathOperator*{\argmaxA}{arg\,max} % Jan Hlavacek

% Extractive sentence compression under lexical and length constraints
\title{Transition-based Sentence Compression with Lexical and Length Constraints}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

% \ahcomment{Proposed revision: In this work we present a new, transition-based framework for extractive sentence compression. We use this framework to construct a sentence compression system based on a very simple neural network, which achieves large gains in automatic evaluation measures over previous state-of-the-art approaches. Unlike prior techniques, our method also allows accommodates lexical and length constraints without incurring exponential computational costs. This makes our method better suited to user-facing search applications, where length and lexical constraints are paramount.}

% \ahcomment{note: gurobi pool search mode is turned ON which makes everythign go slower. you should turn this off if you start making arguments about wall clock time}


\begin{abstract}
Traditional approaches to extractive sentence compression seek to reduce the length of a sentence, while retaining the most ``important'' information from the source. But query-focused applications such as document search engines or exploratory search interfaces place additional lexical and length requirements on compression systems. This study introduces a new transition-based, neural compression method which accommodates such requirements by pruning syntactic dependency subtrees.  We show that our technique is more computationally efficient than previous ILP-based approaches, and achieves superior performance in reconstructing known-good shortenings under constraints.
\end{abstract}

\section{Introduction}

Traditional study of extractive sentence compression seeks to create short, readable compressions which retain the most ``important'' information from source sentences. But query-focused and user-facing applications impose additional requirements on the output of a compression system. Compressions must be short enough to be shown in a user interface and must contain a user's query term, which defines the ``important'' information in the sentence. An example of such a compression is shown in Figure \ref{f:qf}.

This study examines the English-language compression problem with such length and lexical requirements: compressed sentences are (1) required to include a list of query words $q$ and (2) required to be shorter than or equal in length to a maximum character length, $b \in \mathbb{Z}^{+}$. 

\begin{figure}[htb!]
%\fbox{
%\begin{minipage}{7.6 cm}
\includegraphics[width=8.5cm]{qf.pdf}
\caption{A search interface (boxed, top) returns a headline $h$ above a constrained compression $c$, shortened from a sentence $s$ in a query-relevant document (italics, bottom). The constrained compression must contain the users' query terms (bold), and must not exceed 75 characters in length.}
\label{f:qf}
%\end{minipage}
%}
\end{figure}


While older compression methods based on integer linear programming could trivially accommodate such restrictions \cite{clarke2008global,filippova2013overcoming}, recent work adopts a sequence to sequence paradigm which can often reconstruct gold-standard shortenings, but which does not give practitioners such control \cite{filippova2015sentence}. This makes existing sequence to sequence techniques unsuitable for search engines \cite{hearst2009search}, concept map browsers \cite{falke2017graphdocexplore} and new forms of exploratory textual interfaces \cite{marchionini2006exploratory}, where length and lexical constraints are paramount. 

\begin{table*}[htb!]
\begin{tabular}{lccc}
\textbf{Approach} & \textbf{Worst-case complexity} & \textbf{Constrained}  \\ \hline
Sequence to sequence taggers \cite{filippova2015sentence}   & linear              & no         \\   
\textbf{Bottom-up compression (this work)}  & \textbf{linear}     &      \textbf{yes}   \\
ILP    \cite{filippova2013overcoming,Wang2017CanSH}       &   exponential    & yes      \\
\end{tabular}
\caption{Three algorithms for sentence compression. \ahcomment{Add results here too?} Integer linear programming methods \cite{clarke2008global,filippova2013overcoming,Wang2017CanSH} can easily accommodate length and lexical constraints, and can make use of large parallel corpora of sentence--compression pairs. But these methods formulate the compression task as an NP-hard problem (\S\ref{s:ilps}), with worst-case exponential runtime. LSTM taggers \cite{filippova2015sentence} achieve comparable results with a linear runtime, but cannot accommodate length or lexical requirements. This work introduces a supervised, transition-based approach (\S\ref{s:system}) which can be used to compress sentences under lexical and length constraints in linear time.} 
\label{t:algos}
\end{table*}


Therefore, in this work, we present a new method for compressing sentences, which efficiently accommodates lexical and length constraints. Our transition-based, stateful compression method is inspired by similar approaches to transition-based dependency parsing \cite{nivre2003,D14-1082}. We compare this technique to supervised, integer linear programming (ILP) systems, which also accommodate length and lexical restrictions. We show that our method has lower theoretical and empirical computational costs while better reconstructing known-good, constrained shortenings. 

\section{Methods for sentence compression}

This work contrasts traditional ILP-based compression with our novel transition-based framework. Both ILP-based methods and transition-based methods can accommodate constrained compression. We also briefly discuss sequence to sequence compression methods, which do not accommodate length or lexical requirements. Table \ref{t:algos} provides a summary of each approach.

\subsection{ILP-based compression}\label{s:ilps}

One common approach for shortening sentences formulates compression as an ILP task. ILP-based methods assign binary variables to each token in a sentence \cite{clarke2008global} or subtree in a dependency parse \cite{filippova2008dependency}. These variables indicate if the corresponding sentence component is included in a compression. Each such component is also assigned a local weight, indicating its worthiness for inclusion in a shortened sentence. Local weights are either learned from direct supervision \cite{filippova2013overcoming,Wang2017CanSH}, or inferred from sources like corpus statistics, importance score metrics and n-gram language models \cite{clarke2008global,filippova2008dependency}.

Such ILP methods represent the overall quality of a compression by summing the local weights of sentence components to compute a global objective score.  The task of identifying the best possible solution to an integer programming objective is a well-known, NP-hard problem \cite{clarke2008global}. In the worst case, such problems require exponential computation in the length of the input sequence. Researchers use off-the-shelf ILP solvers to identify the highest-scoring compression, from among all possible configurations of binary variables (each may be set to 0 or 1). 

This integer linear programming approach also easily accommodates constrained compression. Researchers will customarily add constraint terms to the ILP objective to enforce hand-build semantic restrictions \cite{clarke2008global} or syntactic requirements \cite{filippova2008dependency}. Adding additional length or lexical requirements to ILPs is straightforward: practitioners must specify that optimal solutions must be shorter than some character budget, and must specify that binary variables marking inclusion of particular words must be set to 1. In practice, we have found that translating a sentence compression objective into an integer linear program can be a challenging and error-prone programming task. These engineering difficulties are a disadvantage of ILPs.

\subsection{Sequence to sequence compression}

We contrast transition-based compression and integer programming approaches with sequence to sequence methods for the compression task \cite{filippova2015sentence}. These techniques achieve high automatic evaluation scores by labeling input tokens with a 1 or a 0, indicating if the word should be included in a shortening. Such methods are also linear in the token length of the input sequence, where ILPs incur exponential cost. However, at this time, sequence to sequence taggers are unsuitable for query-focused applications because such methods cannot enforce lexical or length requirements. This limitation might be reexamined in future work, by modifying or adapting new constrained generation techniques \cite{N18-1119,aaimh}.


\begin{algorithm}[]
\SetKwInOut{Input}{input}
\SetAlgoLined
\Input{$s=(V,E)$, $Q \subseteq V$, $b \in \mathbb{R^+}$}
 $ C \gets Q;  F \gets V \setminus Q$; \\
 \While{ $\ell(C) < b $ and $ |F| > 0 $   }{
  $v \gets \pi(F)$; \\
  $F \gets F \setminus  \{v\}$ \\
  \lIf{$p(v | C) > .5$}{$C \gets C \cup \{v\}$}
 }
\KwRet{$\ell(C)$}
 \caption{Bottom-up compression}
\end{algorithm}


\subsection{Transition-based compression}

In this work, we present a new transition-based method for shortening sentences under lexical and length constraints (\S\ref{s:system}). Like ILP-based techniques, transition-based approaches can easily accommodate lexical and length restrictions. We show that our transition-based method has a lower computational cost (\S\ref{s:costs}), while achieving better performance in constrained compression (\S\ref{s:autoeval}) than the leading, supervised ILP technique \cite{filippova2013overcoming}.

\section{Transition-based sentence compression}\label{s:system}

In this work, we present a new transition-based sentence compression system, inspired by similar approaches in transition-based parsing \cite{nivre2003,D14-1082}. Where some previous approaches shorten sentences by removing components from a well-formed tree from the \textit{top down} \cite{Jing2000SentenceRF,Knight2000StatisticsBasedS,Filippova2015FastKS}, our method constructs a compression of a sentence from the \textit{bottom up}, by adding vertexes to a set of query nodes $Q$ until exhausting a character-based length budget $r \in \mathbb{R}^{+}$. (Query vertexes are leaves at the bottom of dependency parse.)

\subsection{Formal Description}

Our method builds a compression $C$ over at most $|V|$ timesteps by maintaining a state
$(C_i,F_i)$. $C_i$ is the set of tokens in the current compression and $F_i$ is a set of frontier tokens, considered for possible inclusion in the compression at timestep $i$.

We initialize $C_0=Q$ and $F_0 = V \setminus Q$. Then, at each timestep, decide if some vertex $v_i \in F_i$ should be added to $C_i$. We select $v_i$ from $F_i$ according to some policy $\pi$, denoted $v_i = \pi(F_i)$. 

At training time, we use an oracle to decide if $v_i$ should be added to $C_i$, denoted $C_i \gets v_i \cup C_i$: if $v_i$ is included in a gold compression, the vertex is added to $C_i$. At runtime, we use a model of oracle decisions $p(v_i|C_i)$ to determine if a vertex should be included.

In this work we use a rule-based policy $\pi$: neighbors of $C$ are selected from $F$ before non-neighbors. If more than one neighboring or non-neighboring vertex is available in $F$ we proceed in left-to-right order. Learned policies \cite{sutton2018reinforcement} might improve performance, but are left for future work. 

\ahcomment{need a diagram}

\subsection{Oracle paths}\label{s:oracle}

We identified the operations in our compression system empirically: we found that for all compressions in a large, standard corpus \cite{filippova2013overcoming} there exists an oracle path of at most $|s|$ operations which can fully reconstruct the shortened sentence, where $|s|$ denotes the token length of the original sentence.

We identify the oracle path by executing a sequence of operations. \ahcomment{This can be redone/simplified}

\subsection{Modeling}\label{s:modeling}

In our framework (\S\ref{s:oracle}), the oracle compression process is a series of binary decisions. At each step $j$ the oracle must decide to execute (or not add) some proposed vertex $o_j$.

\ahcomment{Add some discussion of features}


%To train the LSTM, we encode both the state of the compression system and the operation proposal into a sequence of input symbols $\bm{x}_j$, called the markup. The LSTM predicts $p(y_j=1 | \bm{x}_j)$, the probability of a binary oracle decision, given the markup. 

%The markup is the deterministic (rather than learned) output of a function $\bm{x}_j=m(V_j,v_j,o_j,s)$, which inserts additional symbols into $s$, the original sequence of tokens in the sentence (see Figure \ref{f:example}). The markup function adds four kinds of symbols.

%\begin{enumerate}
%\item{\textbf{Deletion indicators} tag tokens from $s$ which are not included in the compression at timestep $j$. (These indicators are denoted with strikethrough text in Figure \ref{f:example}.) The markup function concatenates a deletion indicator to all $t_i \in \{s\} \setminus V_j$, where $\{s\}$ denotes the unordered set of tokens from $s$.} 
%\item{\textbf{Sentence brackets} represent the start and end of the original, uncompressed sentence $s$. Sentence brackets are shown with $\langle s \rangle$ and $\langle / s \rangle$ in Figure \ref{f:example}.}
%\item{\textbf{Compression brackets} show the location of a compression within $s$. If the operation proposal is \textsc{Prune}, these brackets show the start and end of the compression $V_j$ within $s$. If the operation proposal is \textsc{Insert}, these brackets show the start and end of the compression $V_{j+1}$, if the proposed operation were to be accepted at timestep $j$. Compression brackets are shown with $\langle c \rangle$ and $\langle / c \rangle$ in Figure \ref{f:example}. 

%In some cases, some of the tokens in the span from $\langle c \rangle$ to $\langle /c \rangle$ might not be in $V_j$. (For instance, a system might have pruned a modifier in this span at an earlier timestep.)  The markup function will tag these tokens (among others) with deletion indicators.}
%\item{\textbf{Subtree brackets} represent the start and end of $T(v)$, defined previously (\S\ref{s:formal}) as the tokens in the subtree which would be pruned or inserted by the operation proposal $o_j$. Subtree brackets have a more complex structure which encodes $(i)$ the type of the operation proposal and $(ii)$ the syntactic role of $T(v)$ within $s$. Figure \ref{f:example} shows the subtree brackets $\langle \textsc{Prune} \texttt{dobj} ? \rangle$ and $\langle / \textsc{Prune} \texttt{dobj} ? \rangle$. Details of the structure of these brackets are provided in the appendix.}
%\end{enumerate}

%Including the three sets of inline bracket symbols, along with the deletion indicator symbols, allows the LSTM to model the relationship between $(a)$ the tokens to be pruned or inserted, $(b)$ the current compression, $(c)$ the operation proposal, and $(d)$ the original sentence $s$. Prior work shows that LSTMs appear capable of modeling bracketing within a sentence \cite{Vinyals2015GrammarAA,karpathy,Aharoni2017TowardsSN}.

%Our architecture (Figure \ref{f:example}) follows recent work on LSTM classification for sentence-level tasks \cite{D17-1070}. Specifically, we predict binary, oracle operations using a Bi-LSTM layer, a max pooling layer, and two separate sequences of fully-connected layers: one for \textsc{Prune} vs. \textsc{NoPrune} decisions, and one for \textsc{Insert} vs. \textsc{NoInsert} decisions. We interpret the shared Bi-LSTM layer as learning deep features from the markup, and each sequence of fully-connected layers as using those features to perform a different kind of binary classification.

%\footnote{The symbols $\langle {o \cdot d} \rangle$ and $\langle / {o \cdot d} \rangle$  allow us to encode some aspects of syntax trees using a vanilla LSTM , without the added complexities of explicitly encoding nested structures into the architecture of the network \cite{Tai2015ImprovedSR,Dyer2016RecurrentNN}.} }

%It lets the LSTM reason about the relationship between the to-prune/insert span, versus the rest of the sentence, by using inline start/end markup symbols. This is motivated by prior work showing that LSTMs are good at modeling inline bracketing within a sentence (CITE: the "parsing as foreign language" paper, and maybe a "LSTMs learn paren balancing" paper, maybe Karpathy or that newer Goldberg one)

 %$(i)$ the first and last  $V_{j + 1}$, if the operation were to be ex

%In this section, we refer to all tokens in $V_j$ as the compression, $c_j$.

% about if these tokens are included or not included in t

%``shows'' the model what a compressed sentence would ``look'' like if the compression system were to execute the operation $o$ given the state $(V, [v|B])$. In the case of insert operations, this means that the markup ``shows'' what the resulting compression would be, if the subtree $T(v)$ were to be copied into $V$. In the case of prune operations, this means that the function ``shows'' what the compression would be if $T(v)$ were to be removed from $V$.

\section{Automatic evaluation of constrained compression}\label{s:autoeval}

We automatically evaluate different approaches to constrained compression, using token-level F1 to measure how well each compression method can reconstruct a known-good shortening. (Token-level F1 is the standard automatic evaluation metric for the compression task.) We also examine the readability of compressions with the automated metric SLOR (\S\ref{s:readabilityinformativeness}). 

Our method better reconstructs known-good compressions, and achieves higher automatic readability scores (Table \ref{t:results}). We evaluate the significance of each difference with bootstrap sampling \cite{D12-1091}. The differences are significant {\small $(p < 10^{-2})$}. 

\subsection{Synthetic constrained compressions dataset}

In order to evaluate different approaches to constrained compression, we require an evaluation dataset of sentences, constraints and known-good shortenings (which respect the constraints). Formally, this means we need input $(s, q, b)$ where $s$ is a sentence, $q$ is a list of query tokens and $b \in \mathbb{Z}^{+}$ is a maximum character budget. For each input triple $(s,q,b)$ we need an output compression $c$, where $c$ is a known-good shortening that $(1)$ includes all tokens from $q$ and $(2)$ is no longer than $b$ (by character length). 

To support large-scale automatic evaluation, we reinterpret 
\citeauthor{filippova2013overcoming}'s
standard compression corpus
as a collection of input triples and output compressions.
In 6,827 of the 10,000 $(s,c)$ pairs in its test set,
$c$ includes one or more named entities.\footnote{We re-tokenize, parse and tag NER spans from the original dataset with Stanford CoreNLP 3.8.0 \cite{corenlp}. We use a standard 3-class definition of NER; for our purposes, entities are people, locations or organizations. Prior work transforms compressions during preprocessing \cite{filippova2013overcoming}. We reimplement these transformations using UD (see Appendix), after parsing and tagging the untransformed compressions from the dataset.}
We interpret these named entities as the query $q$,
and we interpret the length of $c$ (in characters) as the character budget $b$.
This defines our test set of 6,827 tuples, each of the form $(s,q,b,c)$. Each tuple consists of a sentence, a lexical constraint, a budget constraint and a
known-good compression.

\begin{table}[]
\begin{tabular}{lrr}
\centering
Approach & F1 & SLOR  \\ \hline
Query terms only {\small (lower bound)} & 0.381 & -0.088 \\
Supervised ILP  &  0.854   &  0.776       \\
\textbf{Transition-based deletion} &  \textbf{0.875}  & \textbf{0.800}   \\
\end{tabular}
\caption{Test F1 scores for two compression methods, for the constrained compression task. \ahcomment{Add Wall Clock + complexity} We also show scores for SLOR, an automated readability metric (\S\ref{s:readabilityinformativeness}). The difference in token-level F1 scores and SLOR scores for each compression method is statistically significant {\small $(p < 10^{-2})$}. Selecting only query terms for a compression achieves an F1 = 0.381, which is the lower bound for this task.}
\label{t:results}
\end{table}

%Our experiment provides the oracle compression length to each compression system (via the parameter $b$). Automatic performance of compression systems is known to reflect their compression rate \cite{napoles2011evaluating}. We observe that both the ILP and the transition-based compressor achieve higher F1 in the constrained task than in the unconstrained setting.

\subsection{Implementation: transition-based compression}\label{s:transition}

Any details on transition-based compression go here.

%Our model of transition-based sentence compression (\S\ref{s:modeling}) predicts the oracle operation $y$, given the state $(V,[v|B])$. There are many possible ways to use $p(y=1| \bm{x})$ in a query-focused and length-constrained compression system. In this work, we use a simple, greedy, iterative \textbf{transition-based deletion} technique: at each step $j$ we \textsc{Prune} the subtree rooted at 

%$$\argmaxA_{v \in V,q\not\in T(v)}   p(y_j = 1 | \bm{x}_j)$$

%\noindent where $\bm{x}_j$ is the markup and $y_j$ is the binary decision to perform a \textsc{Prune} or \textsc{NoPrune} operation (\S\ref{s:modeling}). We continue pruning subtrees until the length of the linearized tokens in $V$ is less than $b$, in which case we stop compression. 

%We initialize $(V, [v|B])$ with the smallest subtree from the dependency parse of $s$ which is (1) rooted at a verb and (2) contains all of the tokens in $q$. For roughly two-thirds of sentences $V$ is simply equal to all tokens in the original sentence. In the remaining cases, all tokens in $q$ are contained in some subclause of the sentence; the compression is formed by shortening this subclause, instead of shortening the whole sentence. In more than 95\% of sentences, the compression system must make additional \textsc{Prune} decisions after initializing with the smallest subtree.

\subsection{Implementation: ILP-based compression}\label{s:ilp}

We compare our system to a state-of-the-art, ILP-based method, presented in \citet{filippova2013overcoming}. This approach proposes representing each edge in a syntax tree with a vector of binary features, then learning weights for each feature using a structured perceptron trained on a corpus of $(s,c)$ pairs. Learned weights are used to compute a global compression objective, subject to structural constraints which ensure the output is a valid tree.

To our knowledge, a public implementation of this method does not exist. We reimplement from scratch using \citet{gurobi}, achieving a test-time, token-level F1 score of  0.690 on the unconstrained compression task. The F1 score in our reimplementation is lower than the than the result reported by the original authors. There are some important differences between our reimplementation and the method reported in \citet{filippova2013overcoming}. We describe these differences in detail in the appendix.

\subsection{Importance and readability evaluation}\label{s:readabilityinformativeness}

Researchers often use human judgements of \textit{importance} and \textit{readability} to evaluate extractive sentence compression techniques \cite{Knight2000StatisticsBasedS,clarke2008global,filippova2015sentence}. 

In a traditional importance evaluation, humans judge the degree to which a compression retains the most ``important'' information from a source sentence. However, in our constrained compression setting, a user's query determines the ``important'' information from $s$, which must be included in a compression. Thus, human importance evaluations are inappropriate.
 
We use the automated SLOR metric \cite{lau2015unsupervised} to check the readability of compressions. Prior work shows that this metric correlates with human readability judgements for the compression task \cite{kannConl}. SLOR normalizes the probability of a token sequence assigned from a language model, by adjusting for both the probability of the individual unigrams in the sentence and for the sentence length.\footnote{Longer sentences are always less probable than shorter sentences; rarer words make a sequence less probable.} Our method achieves a slightly higher SLOR score, which indicates that it might produce slightly more readable compressions (Table \ref{t:results}). The appendix contains additional details of our implementation of SLOR. 

\section{Computational costs of transition-based compression}\label{s:costs}

ILP-based approaches to sentence compression formalize the task as an integer linear programming optimization task, a well-known NP-hard problem with exponential worst-case complexity (\S\ref{s:ilps}).\footnote{The complexity is exponential in $|V|$ if the ILP selects tokens like \citet{clarke2008global}, and exponential in $|E|$ if the ILP selects edges like \citet{filippova2008dependency} or \citet{filippova2013overcoming}.} One advantage of our transition-based framework is that it can perform query-focused, budget-constrained compression that is linear in $|V|$, the token length of $S$. We initialize the frontier $F$ with all $v_i \in V \setminus Q$. We then remove one vertex $v_i$ from $F$ at each timestep. Therefore, our approach must evaluate no more than $|V|$ vertexes to generate a compression. 

\subsection{Empirical costs}\label{s:empiricalcost}

Practical ILP systems \ahcomment{some sort of comment on how it works}. However, the theoretical advantage of our system translates to a practical speed.

\section{Applications and related work}

\subsection{Applications}

Traditional study of sentence compression is motivated by text summarization techniques, which create synopses by selecting and (sometimes) shortening sentences \cite{Knight2000StatisticsBasedS,vanderwende2007beyond,martins2009summarization}.  Our concern with constrained compression is better suited to search applications, in which user input defines important information in source documents. For instance, constrained compression could be used to generate query-biased snippets on a search engine results page \cite{tombros1998advantages,Metzler2008MachineLS,kanungo2009predicting}.\footnote{We note that version 7.5 of Apache Lucene, the leading open source search engine, does not perform sentence compression in generating snippets. \url{https://lucene.apache.org/core/7_5_0/highlighter/index.html}} Constrained compressions could also be used as part of new forms of search user interfaces \cite{hearst2009search}, such as concept map browsers \cite{falke2017graphdocexplore}. Particular forms of query-focused summarization, like summarizing people \cite{w04} or companies \cite{filippova2009company}, also require compressions with hard lexical constraints. 

\subsection{Related work}

To our knowledge, this work is the first study of length and lexically constrained compression.\footnote{\citet{Li2013DocumentSV} solicit annotations for ``guided'' compression, in which humans are directed to include particular words in manually shortening sentences, but do not examine the compression problem under lexical constraints.} We formalize the problem as a strictly extractive task, following a line of research in which compressions are formed by deleting tokens from an original sentence \cite{clarke2008global,filippova2008dependency,filippova2015sentence}. Other approaches compress sentences by generating instead of deleting words \cite{rush2015neural,mallinson18}. \ahcomment{not ready for users. cite zhang} Our transition-based framework might be extended with abstractive operations in future work, following recent interest in blending summarization techniques \cite{P17-1099}.

\section{Conclusion and future work}

This work introduces a new, neural, transition-based method for extractive sentence compression. Our approach is both more computationally efficient than ILP-based methods, and better reconstructs known-good sentence shortenings under constraints. 

%Finally, some shortened sentences will modify the meaning of a sentence. Identifying such cases is a special case of the unsolved textual entailment problem \cite{snli_bowman,Pavlick2016SoCalledNA,linzencompression}. In the future, we plan to apply entailment research to the compression task.  

%Finally, our method performs sequential decision making by learning from oracle path operations at training time. When we use this oracle guidance to compress sentences, our transition-based compressor inevitably makes mistakes and diverges from the oracle path, limiting the usefulness of training data. Training with examples which diverge from the oracle path could improve performance in future work.

% ack => Katie, Javier, NLP reading group! 

%\include{appendix_content}

\bibliography{abe}
\bibliographystyle{acl_natbib}

\end{document}

%\ahcomment{Sort of hard to tell what formalism F and A use. I thnk it is stanford, but they don't come out and say it.They cite Nivre's book which references the malt parser which seems to use stanford deps. but I don't see mention of the ``in" relation referenced in F and A paper in a guide to stanford deps. Writing around it.}

% other ideas... 
 
%\section{Computational experiments part 2: investigating properties of q,s,r compression}

%\ahcomment{include?}

%\ahcomment{only outline / notes here}

%\begin{enumerate}
%\item{q = a list of 1 to 3 NER}
%\item{r = random}
%\item{What is the size of the minimum compression?}
%\item{Reachability by budget by position of q in syntax tree. (If q is more than one entity then how the entities are dispersed across the tree probably matters a bunch too).}%
%\item{Hang on. reachability == min compression, eh? if min compression $>$ b, it is clearly bad.}
%\item{Avoid computational waste w/ grammar.  Examine: ops you never have to worry about if you prune a branch v. dependency type deletion endorsement rate. Some ops get rid of lots of tokens w/ very high probability of deletion endorsements: e.g. parataxis (a great op!). By contrast: pruning a noun subj destroys acceptability and usually does not delete many tokens. Not worth the risk!}
%\item{What is the empirical number of ops (i.e. decisions you have to make about pruning) if you greedily drop branches but never drop if the single op probability is less than $p$? My guess is you can make this problem way, way, simpler than implied by exponential formulation. Is it really quadratic?}
%\item{Distribution of number of ops used for different q and r: when choosing ops at random? when choosing greedily? When pruning $\propto$ p(endorsement)?}
%\item{Other stuff: min compression, reachability, operations saved w/ big prunes? position of query in the tree?}
%\end{enumerate}
