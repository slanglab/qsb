%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\newcommand{\rdep}[1]{\ $\xrightarrow{\text{\tiny #1}}$\ }


\newcommand{\ahcomment}[1]{\textcolor{blue}{[#1 -AH]}}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\hypersetup{draft} % THIS IS NEEDED TO GET IT TO COMPILE. Does not like the tables. AH 11/27

\newcommand\BibTeX{B{\sc ib}\TeX}

\DeclareMathOperator*{\argmaxA}{arg\,max} % Jan Hlavacek

% Extractive sentence compression under lexical and length constraints
\title{Additive Compression with Lexical and Length Constraints}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Extractive sentence compression seeks to reduce the length of a sentence, while retaining the most ``important'' information from the source. But query-focused applications such as document search engines or exploratory search interfaces place additional length, lexical and latency constraints on compression systems. This study introduces a new transition-based technique, developed for such settings. Our method constructs compressions in linear time by growing a forest in a dependency parse, achieving a 100X speed up over traditional ILP-based methods. Our technique also better reconstructs known-good shortenings under constraints.
\end{abstract}

\section{Introduction}\label{s:intro}

Traditional study of extractive sentence compression seeks to create short, readable compressions which retain the most ``important'' information from source sentences. But query-focused and user-facing applications impose additional requirements on the output of a compression system. Compressions must be short enough to be shown in a user interface and often must contain a user's query term, which defines the ``important'' information in the sentence. Query-focused applications applications also require low latency to support interactive browsing. An example application is shown in Figure \ref{f:qf}.

\begin{figure}[htb!]
%\fbox{
%\begin{minipage}{7.6 cm}
\includegraphics[width=8.5cm]{qf.pdf}
\caption{A search interface (boxed, top) returns a headline $h$ above a constrained compression $c$, shortened from a sentence $s$ in a query-relevant document (italics, bottom). The constrained compression must contain the users' query terms (bold), and must not exceed 75 characters in length.}
\label{f:qf}
%\end{minipage}
%}
\end{figure}

This study examines the English-language compression problem with such length and lexical requirements. In constrained compression, source sentences $S$ are shortened to compressions $C$ which (1) include query words $Q$ and (2) are shorter than or equal in length to a maximum character length, $b \in \mathbb{Z}^{+}$. Formally, the task is to construct a function $(S,Q,b) \rightarrow c$, such that $c$ respects $Q$ and $b$.

Older compression methods  \cite{clarke2008global,filippova2013overcoming} based on integer linear programming can trivially accommodate such restrictions, but rely on slow third-party solvers to optimize an NP-hard integer linear programming objective\label{s:relatedwork}. Newer sequence to sequence methods \cite{filippova2015sentence} do not allow practitioners to specify length or lexical constraints, and require access to specialized graphics processing hardware (GPU) to achieve low latency which is a barrier for practitioners in fields like social science and journalism. These deficits make existing compression techniques unsuitable for search engines \cite{hearst2009search}, concept map browsers \cite{falke2017graphdocexplore} and new forms of exploratory textual interfaces \cite{marchionini2006exploratory}, where length, lexical and latency constraints are paramount. 

\begin{table*}[htb!]
\begin{tabular}{lccc}
\textbf{Approach} & \textbf{Worst-case complexity} & \textbf{Constrained}  \\ \hline
Sequence to sequence taggers \cite{filippova2015sentence}   & linear              & no         \\   
\textbf{Additive compression (this work)}  & \textbf{linear}     &      \textbf{yes}   \\
ILP    \cite{filippova2013overcoming,Wang2017CanSH}       &   exponential    & yes      \\
\end{tabular}
\caption{Three algorithms for sentence compression. Integer linear programming methods \cite{clarke2008global,filippova2013overcoming,Wang2017CanSH} can easily accommodate length and lexical constraints, and can make use of supervision. But these methods formulate the compression task as an NP-hard problem with worst-case exponential runtime. LSTM taggers \cite{filippova2015sentence} achieve comparable results with linear runtime, but cannot accommodate length or lexical requirements. This work introduces a supervised additive approach (\S\ref{s:system}) which can be used to compress sentences under lexical and length constraints in linear time.} 
\label{t:algos}
\end{table*}

We thus present a new method for constrained compression which grows a forest in a dependency parse in linear time. We compare this \textit{additive compression} method to supervised, integer linear programming (ILP) systems, which also accommodate length and lexical restrictions. Our method has lower theoretical and empirical computational costs, while better reconstructing known-good shortenings. 

\section{Related work}\label{s:relatedwork}

Extractive compression shortens a sentence by removing tokens \cite{Knight2000StatisticsBasedS,clarke2008global,filippova2015sentence,Wang2017CanSH}, typically for extractive summarization \cite{Knight2000StatisticsBasedS,almeida2013fast,P16-1188}.\footnote{Some methods shorten sentences via generation instead of deletion \cite{rush2015neural,mallinson18}. Our interest in the extractive setting follows from a motivation to create interpretable,  trustworthy, and practical search systems \cite{Chuang2012InterpretationAT}: users might not trust abstractive summaries \cite{Zhang:2018:MSG:3290265.3274465}, particularly in cases with semantic error.} To our knowledge, this work is the first to consider extractive compression under length and lexical constraints.\footnote{\citet{Li2013DocumentSV} solicit annotations for ``guided'' compression, but do not examine the compression problem under lexical and length constraints.}

    Our interest in constrained compression is motivated by search user interfaces \cite{hearst2009search}, which require query-biased snippets for search engine results pages \cite{tombros1998advantages}\footnote{Version 7.5 of Apache Lucene, the leading open source search engine, does not perform sentence compression in generating snippets. \url{https://lucene.apache.org/core/7_5_0/highlighter/index.html}}. New forms of search interfaces such as concept map browsers could also make use of constrained compressions \cite{marchionini2006exploratory,emnlp2017conceptmaps}. Because such user-facing settings require low-latency techniques \cite{Nielsen,heerschei,Liu2014TheEO}, we prioritized computational efficiency in designing a solution to the compression task. We also avoided neural approaches which achieve low latency with an expensive GPU.

We compare our approach to ILP compression techniques \cite{clarke2008global,filippova2008dependency,filippova2013overcoming,Wang2017CanSH}, which can easily specify that optimal solutions need to be shorter than some character budget, and need to include query terms. However, the task of identifying the global optimum of such an integer programming objective is a well-known, NP-hard problem \cite{clarke2008global}, requiring worst-case exponential computation in the length of the input sequence. In practice, ILP methods use off-the-shelf solvers to identify the highest-scoring compression, from among all possible configurations of binary variables. 

At this time, sequence to sequence sentence compression \cite{filippova2015sentence} is unsuitable for query-focused applications because such methods cannot enforce lexical or length requirements. This limitation might be reexamined in future work, by modifying or adapting new constrained generation techniques \cite{N18-1119,aaimh}.

\section{Additive compression}\label{s:system}

In this work, we present a new transition-based method for shortening sentences under lexical and length constraints (\S\ref{s:system}), inspired by similar approaches in transition-based parsing \cite{nivre2003,D14-1082}. We describe our technique as additive compression, because it constructs a compression by \textit{growing} a forest in the dependency parse of a sentence. This approach differs from some prior work \cite{Jing2000SentenceRF,Knight2000StatisticsBasedS,berg2011jointly,almeida2013fast,filippova2015sentence}, which
shortens a sentence by \textit{pruning} subtrees. 

The chief advantage of additive compression is that it can construct constrained compressions in linear rather than exponential time (\S\ref{s:costs}). Such theoretical gains translate to actual lower computational cost (\S\ref{s:costs}), while achieving better performance in constrained compression (\S\ref{s:autoeval}) than supervised ILP techniques.

\begin{algorithm}[]
\SetKwInOut{Input}{input}
\SetAlgoLined
\Input{$s=(V,E)$, $Q \subseteq V$, $b \in \mathbb{R^+}$}
 $ C \gets Q;  F \gets V \setminus Q$; \\
 \While{ $\ell(C) < b $ and $ |F| > 0 $   }{
  $v \gets \pi(F)$; \\
  $F \gets F \setminus  \{v\}$ \\
  \If{$p(v | C) > .5$* and $\ell(C \cup \{v\}) < b$}{$C \gets C \cup \{v\}$}
 }
\KwRet{$\ell(C)$}
 \caption{Additive compression {\small(test time*)}}
\end{algorithm}\label{a:algo}

\subsection{Formal Description}\label{s:formal}

Our method builds a compression by maintaining a state
$(C,F)$ over at most $|S|$ timesteps, where $C$ is a set of tokens from $S$ which will be included in the final compression and $S$ is the set of tokens from the original sentence. $F$ is a set of frontier tokens.

During initialization, we set $C \gets Q$ and $F \gets V \setminus Q$. Then, at each timestep, we decide to either add or not add some vertex $v \in F$ to $C$. We such discuss decisions in detail in \S\ref{s:modeling}. At each timestep, we also remove $v$ from $F$. We continue adding vertexes to $C$ until either $F$ is empty or the linearized compression, $\ell(C)$, is longer than the length constraint.\footnote{We linearize tokens in their original sentence order. \ahcomment{explain}} Figure \ref{f:walkthru} shows a step-by-step example.

We select $v$ from $F$ according to some policy $\pi$, denoted $v = \pi(F)$. In this work we use a rule-based policy: neighbors of $C$ are selected from $F$ before non-neighbors. If more than one neighboring or non-neighboring vertex is available in $F$, we proceed in left-to-right order.

\begin{figure}[h]
\includegraphics[width=8cm]{sample.jpg}
\caption{Additive compression in 4 timesteps, where $Q$=$\{1,2\}$. $y$ indicates the decision at each timestep.}
\end{figure}\label{f:walkthru}


\subsection{Inclusion decisions: oracle and modeling}\label{s:modeling}

We use an oracle to decide if a given $v$ should be added to the compression: if $v$ is included in a gold compression, the vertex is added to $C$. This method of determining paths based on oracle decisions can reconstruct all shortenings in a large, standard compression corpus \cite{filippova2013overcoming}.

We use such oracle paths to train a model of oracle inclusion decisions, $p(y  = 1 | v, C, S)$. After initially implementing a neural approach \cite{D14-1082}, we instead chose to instead implement feature-based, binary logistic regression which can run quickly without access to a GPU. This allows our method to be used in search applications in fields like social science and journalism, where expensive custom hardware is not available. 

The features in our model fall into two classes. 

\textbf{Local features} describe the properties of the edge $(u,v)$ between $v$ and $u \in C$. Such features are based on previous approaches to supervised non-neural sentence compression \cite{filippova2013overcoming,almeida2013fast,Filippova2015FastKS}, including structural features (e.g.~vertex depth in tree), lexical features, and semantic features (e.g.~NER tag). Unlike previous work, our local features indicate if $u$ governs $v$ or a dependent of $v$. This allows our model to reason about the sorts of governors which should be included in a compression (e.g.~verbs) and the sorts of dependents which might not be added to $C$ (e.g.~modifiers). We describe the features in detail in our appendix and supporting code. (If $v$ is disconnected from $C$, no local features are used.)  

\textbf{Global features} represent the relationship between $v$ and the overall compression $C$. Global features include information such as the position of $v$ in the sentence, relative to the right-most and left-most vertex in $C$. Such global features allow the feature-based model to reason about which sort of vertexes should be added to compressions in general. For instance, global features allow the model to learn if the oracle tends to include or not include gaps.

\ahcomment{credit that k path one}

\ahcomment{Describe oracle paths}

\section{Constrained compression: Automatic Evaluation}\label{s:autoeval}

We automatically evaluate different approaches to constrained compression, using token-level F1 to measure how well each compression method can reconstruct a known-good shortening. (Token-level F1 is the standard automatic evaluation metric for the compression task.) We also examine the readability of compressions with the automated metric SLOR (\S\ref{s:readabilityinformativeness}), and analyze both theoretical complexity and wall clock speed (\S\ref{s:costs}).

Our method better reconstructs known-good compressions, and achieves higher automatic readability scores (Table \ref{t:results}). We evaluate the significance of each difference with bootstrap sampling \cite{D12-1091}. The differences are significant {\small $(p < 10^{-2})$}. 

\subsection{Synthetic constrained compression experiment}

In order to evaluate different approaches to constrained compression, we require an evaluation dataset of sentences, constraints and known-good shortenings (which respect the constraints). Formally, this means we need input $(s, q, b)$ where $s$ is a sentence, $q$ is a list of query tokens and $b \in \mathbb{Z}^{+}$ is a maximum character budget. For each input triple $(s,q,b)$ we need an output compression $c$, where $c$ is a known-good shortening that $(1)$ includes all tokens from $q$ and $(2)$ is no longer than $b$ (by character length). 

To support large-scale automatic evaluation, we reinterpret 
\citeauthor{filippova2013overcoming}'s
standard compression corpus
as a collection of input triples and output compressions.
In 6,827 of the 10,000 $(s,c)$ pairs in its test set,
$c$ includes one or more named entities.\footnote{We re-tokenize, parse and tag NER spans from the original dataset with Stanford CoreNLP 3.8.0 \cite{corenlp}. We use a standard 3-class definition of NER; for our purposes, entities are people, locations or organizations. Prior work transforms compressions during preprocessing \cite{filippova2013overcoming}. We reimplement these transformations using UD (see Appendix), after parsing and tagging the untransformed compressions from the dataset.}
We interpret these named entities as the query $q$,
and we interpret the length of $c$ (in characters) as the character budget $b$.
This defines our test set of 6,827 tuples, each of the form $(s,q,b,c)$. Each tuple consists of a sentence, a lexical constraint, a budget constraint and a
known-good compression.

Prior empirical studies of user search behavior have found that search queries are often short (several tokens long), and often restricted to particular parts of speech (nouns, proper nouns and adjectives). To construct a realistic experiment, we sample query tokens from compressions to match the observed distribution of token counts \cite[table 6] {Jansen2000RealLR}, and observed distribution of part-of-speech tags \cite{Barr2008TheLS} from real-world searches.\footnote{We reject samples in which the query length is longer than 6 tokens, which are poorly suited our setting. We manually map part-of-speech tags from \citet{Barr2008TheLS} to Penn tags, described in supporting code. We reject sampled part-of-speech tags which occur in fewer than 1\% of cases.}

\begin{table}[]
\begin{tabular}{lrrl}
\centering
Approach & F1 & SLOR &  Speed {\small (ms / $s$)}  \\ \hline
Supervised ILP &  {\small 0.854}   &  {\small 0.776 }  & {\small $\mu=$ .115} ({\small $ \sigma=.074$}) \\
Additive compression &  {\small \textbf{0.875}}  & {\small \textbf{0.800} }& {\small $\mu=$ 300} ({\small $ \sigma=X$}) \\
Random addition  &  {\small 0.690}  & {\small 0.800 }& {\small $\mu=$ 300} ({\small $ \sigma=X$}) \\
\end{tabular}
\caption{Test F1 scores, SLOR scores and average runtimes for two compression methods, for the constrained compression task. The difference in token-level F1 scores and SLOR scores for each compression method is statistically significant {\small $(p < 10^{-2})$}. Additive compression with random decisions achieves F1 = 0.690, which is the lower bound for this task.}
\label{t:results}
\end{table}

%Our experiment provides the oracle compression length to each compression system (via the parameter $b$). Automatic performance of compression systems is known to reflect their compression rate \cite{napoles2011evaluating}. We observe that both the ILP and the transition-based compressor achieve higher F1 in the constrained task than in the unconstrained setting.

\subsection{Implementation: transition-based compression}\label{s:transition}

Any details on transition-based compression go here.

%Our model of transition-based sentence compression (\S\ref{s:modeling}) predicts the oracle operation $y$, given the state $(V,[v|B])$. There are many possible ways to use $p(y=1| \bm{x})$ in a query-focused and length-constrained compression system. In this work, we use a simple, greedy, iterative \textbf{transition-based deletion} technique: at each step $j$ we \textsc{Prune} the subtree rooted at 

%$$\argmaxA_{v \in V,q\not\in T(v)}   p(y_j = 1 | \bm{x}_j)$$

%\noindent where $\bm{x}_j$ is the markup and $y_j$ is the binary decision to perform a \textsc{Prune} or \textsc{NoPrune} operation (\S\ref{s:modeling}). We continue pruning subtrees until the length of the linearized tokens in $V$ is less than $b$, in which case we stop compression. 

%We initialize $(V, [v|B])$ with the smallest subtree from the dependency parse of $s$ which is (1) rooted at a verb and (2) contains all of the tokens in $q$. For roughly two-thirds of sentences $V$ is simply equal to all tokens in the original sentence. In the remaining cases, all tokens in $q$ are contained in some subclause of the sentence; the compression is formed by shortening this subclause, instead of shortening the whole sentence. In more than 95\% of sentences, the compression system must make additional \textsc{Prune} decisions after initializing with the smallest subtree.

\subsection{Implementation: min compression baseline? }\label{s:transition}
\ahcomment{some discussion of headlines}

\subsection{Implementation: ILP-based compression}\label{s:ilp}

We compare our system to a state-of-the-art, ILP-based method, presented in \citet{filippova2013overcoming}. This approach proposes representing each edge in a syntax tree with a vector of binary features, then learning weights for each feature using a structured perceptron trained on a corpus of $(s,c)$ pairs. Learned weights are used to compute a global compression objective, subject to structural constraints which ensure the output is a valid tree.

To our knowledge, a public implementation of this method does not exist. We reimplement from scratch using \citet{gurobi}, achieving a test-time, token-level F1 score of  0.690 on the unconstrained compression task. The F1 score in our reimplementation is lower than the than the result reported by the original authors. There are some important differences between our reimplementation and the method reported in \citet{filippova2013overcoming}. We describe these differences in detail in the appendix.

\subsection{Importance and readability evaluation}\label{s:readabilityinformativeness}

Researchers often use human judgements of \textit{importance} and \textit{readability} to evaluate extractive sentence compression techniques \cite{Knight2000StatisticsBasedS,clarke2008global,filippova2015sentence}. 

In a traditional importance evaluation, humans judge the degree to which a compression retains the most ``important'' information from a source sentence. However, in our constrained compression setting, a user's query determines the ``important'' information from $s$, which must be included in a compression. Thus, human importance evaluations are inappropriate.
 
We use the automated SLOR metric \cite{lau2015unsupervised} to check the readability of compressions. Prior work shows that this metric correlates with human readability judgements for the compression task \cite{kannConl}. SLOR normalizes the probability of a token sequence assigned from a language model, by adjusting for both the probability of the individual unigrams in the sentence and for the sentence length.\footnote{Longer sentences are always less probable than shorter sentences; rarer words make a sequence less probable.} Our method achieves that it might produce slightly more readable compressions (Table \ref{t:results}). The appendix contains additional details of our implementation of SLOR. 

\subsection{Computational costs of transition-based compression}\label{s:costs}

ILP-based approaches to sentence compression formalize the task as an integer linear programming optimization task, a well-known NP-hard problem  (\S\ref{s:relatedwork}), with exponential worst-case complexity.\footnote{The complexity is exponential in $|V|$ if the ILP selects tokens like \citet{clarke2008global}, and exponential in $|E|$ if the ILP selects edges like \citet{filippova2008dependency} or \citet{filippova2013overcoming}.} One advantage of our transition-based framework is that it can perform query-focused, budget-constrained compression that is linear in $|V|$, the token length of $S$. Compression is linear because we initialize the frontier $F$ with all $v_i \in V \setminus Q$, and then remove one vertex $v_i$ from $F$ at each timestep. For this reason, our approach must evaluate no more than $|V|$ vertexes to generate a compression. 

We test if the theoretical advantage of our system translates to a practical speedup by sampling 10,000 sentences with replacement from the test set, and performing compression with an ILP. We then repeat this experiment with our additive compression approach (Table \ref{t:results}). Our linear additive technique is \ahcomment{TODO} faster than existing methods. The appendix contains additional computational details.

\ahcomment{query bias in the experiment setup?}

\ahcomment{does it EVER connect a disconnected tree?}

\section{Conclusion and future work}

This work introduces a new, neural, transition-based method for extractive sentence compression. Our approach is both more computationally efficient than ILP-based methods, and better reconstructs known-good sentence shortenings under constraints. \ahcomment{Some comment on connection to headline method. both specify required tokens}

%Finally, some shortened sentences will modify the meaning of a sentence. Identifying such cases is a special case of the unsolved textual entailment problem \cite{snli_bowman,Pavlick2016SoCalledNA,linzencompression}. In the future, we plan to apply entailment research to the compression task.  

%Finally, our method performs sequential decision making by learning from oracle path operations at training time. When we use this oracle guidance to compress sentences, our transition-based compressor inevitably makes mistakes and diverges from the oracle path, limiting the usefulness of training data. Training with examples which diverge from the oracle path could improve performance in future work.

% ack => Katie, Javier, NLP reading group! 

%\include{appendix_content}

\bibliography{abe}
\bibliographystyle{acl_natbib}

\end{document}

%\ahcomment{Sort of hard to tell what formalism F and A use. I thnk it is stanford, but they don't come out and say it.They cite Nivre's book which references the malt parser which seems to use stanford deps. but I don't see mention of the ``in" relation referenced in F and A paper in a guide to stanford deps. Writing around it.}

% other ideas... 
 
%\section{Computational experiments part 2: investigating properties of q,s,r compression}

%\ahcomment{include?}

%\ahcomment{only outline / notes here}

%\begin{enumerate}
%\item{q = a list of 1 to 3 NER}
%\item{r = random}
%\item{What is the size of the minimum compression?}
%\item{Reachability by budget by position of q in syntax tree. (If q is more than one entity then how the entities are dispersed across the tree probably matters a bunch too).}%
%\item{Hang on. reachability == min compression, eh? if min compression $>$ b, it is clearly bad.}
%\item{Avoid computational waste w/ grammar.  Examine: ops you never have to worry about if you prune a branch v. dependency type deletion endorsement rate. Some ops get rid of lots of tokens w/ very high probability of deletion endorsements: e.g. parataxis (a great op!). By contrast: pruning a noun subj destroys acceptability and usually does not delete many tokens. Not worth the risk!}
%\item{What is the empirical number of ops (i.e. decisions you have to make about pruning) if you greedily drop branches but never drop if the single op probability is less than $p$? My guess is you can make this problem way, way, simpler than implied by exponential formulation. Is it really quadratic?}
%\item{Distribution of number of ops used for different q and r: when choosing ops at random? when choosing greedily? When pruning $\propto$ p(endorsement)?}
%\item{Other stuff: min compression, reachability, operations saved w/ big prunes? position of query in the tree?}
%\end{enumerate}
